{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Retail Theft & Organized Crime Trends Project**\n",
        "**Introduction:-** Retail theft has become a growing concern in major cities across the United States, and Chicago is no exception. In recent years, retail giants like Target, Walmart, and Walgreens have publicly reported store closures due to rising theft and organized retail crime (ORC). The impact goes beyond just financial losses—businesses are forced to reduce operating hours, consumers face higher product prices, and law enforcement agencies struggle to keep up with increasing theft incidents.\n",
        "\n",
        "As an aspiring data analyst, I wanted to explore this issue from a data-driven perspective. The goal of this project is to analyze real crime data from the City of Chicago, understand the trends behind retail theft, and provide meaningful insights that could help businesses, law enforcement, and policymakers make better decisions."
      ],
      "metadata": {
        "id": "C92BTpFB9tNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Real-World Scenario: What Sparked My Interest in This Project?** A few months ago, I visited a retail store in downtown Chicago, and while shopping, I overheard employees discussing an increase in theft cases. One of them mentioned how they had to lock up high-value products because shoplifting incidents were happening almost daily. This made me wonder:\n",
        "\n",
        "*   Which areas in Chicago experience the highest number of retail theft incidents?\n",
        "*   Is retail theft seasonal? Do incidents increase during holidays or economic downturns?\n",
        "\n",
        "*   Are arrests happening, or are most offenders walking away without consequences?\n",
        "*   How do these incidents impact businesses and local communities?\n",
        "\n",
        "This personal experience motivated me to dig deeper into public crime data and see what the numbers reveal about the retail theft crisis in Chicago.\n",
        "\n",
        "With these questions in mind, I turned to data for answers. By analyzing real crime data from the City of Chicago, I aimed to uncover patterns, visualize trends, and derive actionable insights. To do this, I leveraged Python for data cleaning, exploratory analysis, and visualization—transforming raw crime reports into meaningful insights. Let’s dive into the data and start exploring."
      ],
      "metadata": {
        "id": "g8ujREQy-DFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Google Drive module\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access files stored in my Drive\n",
        "# This allows my to read and write files between Colab and Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TmEA7pOZ-ywQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries (Run this only once in Google Colab)\n",
        "# folium → Used for creating interactive maps\n",
        "# matplotlib → Used for data visualization (plots & graphs)\n",
        "# pandas → Used for data manipulation and analysis\n",
        "!pip install folium matplotlib pandas\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd  # For handling data (loading, cleaning, and analyzing)\n",
        "import matplotlib.pyplot as plt  # For plotting charts and visualizing data\n",
        "import folium  # For interactive map visualization\n",
        "from folium.plugins import HeatMap  # For creating heatmaps of crime locations\n",
        "import warnings  # For handling warnings\n",
        "\n",
        "# Turn off warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mhzatxrV_T5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "file_path = \"/content/drive/MyDrive/PERSONAL_PROJECT/Retail_Theft_Analysis_Project/Retail_Theft_20250318.csv\"\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "VzctupUI_jcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basic Data Exploration :-** Here I'd successfully loaded the dataset, the next step is to understand its structure, quality, and key characteristics. Before diving into analysis, it’s important to explore the data."
      ],
      "metadata": {
        "id": "y42ob7TkBHJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display basic information about the dataset\n",
        "print(\"Dataset Info:\")\n",
        "df.info()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tOdko-38Ek8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first 5 rows of the dataset\n",
        "df.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TEmZVNVnE0PL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the last 5 rows of the dataset\n",
        "df.tail()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cooPB2gtF2jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display all column names\n",
        "print(\"Column Names:\")\n",
        "print(df.columns)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jD6ipIWBF54z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique values in 'Primary Type' column\n",
        "print(df['Primary Type'].unique())\n",
        "\n",
        "# Unique values in 'Location Description' column\n",
        "print(df['Location Description'].unique())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-WVTc_AIF8Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(\"Missing Values in Each Column:\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mQ-vu07AE45Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary statistics for numerical columns\n",
        "print(\"Summary Statistics:\")\n",
        "print(df.describe())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qN6Eg19HFABX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count of incidents per year\n",
        "print(\" Number of Retail Theft Cases Per Year:\")\n",
        "print(df['Year'].value_counts())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "c2GERTvKFMnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt  # Import the Matplotlib library for creating visualizations\n",
        "import seaborn as sns  # Import Seaborn for advanced data visualization\n",
        "\n",
        "# Create a figure with a specified size (10 inches wide, 5 inches tall)\n",
        "plt.figure(figsize=(10,5))\n",
        "\n",
        "# Plot a histogram to visualize the distribution of theft cases by year\n",
        "sns.histplot(df['Year'], bins=15, kde=True, color='blue')\n",
        "\n",
        "# Set the title of the plot\n",
        "plt.title(\"Distribution of Retail Theft Cases Over the Years\")\n",
        "\n",
        "# Label the x-axis to indicate it represents years\n",
        "plt.xlabel(\"Year\")\n",
        "\n",
        "# Label the y-axis to indicate it represents the count of theft cases\n",
        "plt.ylabel(\"Count of Cases\")\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MBjriOHbFVUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Understanding the Need for Data Cleaning :- ***After exploring the dataset, we can see that some values are missing, and certain columns might need formatting adjustments. Raw data is often incomplete or inconsistent, which can impact the accuracy of our analysis.\n",
        "\n",
        "Before proceeding with in-depth insights, we need to clean the dataset by handling missing values, converting date formats, and ensuring all fields are structured correctly. This step is crucial to ensure reliable and meaningful analysis.\n",
        "\n",
        "Let’s move forward with data cleaning to prepare the dataset for deeper exploration and visualization."
      ],
      "metadata": {
        "id": "0GGehBHhHZ3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Handle Missing Values\n",
        "# Fill missing values in categorical columns with \"Unknown\"\n",
        "df = df.assign(\n",
        "    **{\"Location Description\": df['Location Description'].fillna(\"Unknown\"),\n",
        "       \"Ward\": df['Ward'].fillna(df['Ward'].mode()[0]),\n",
        "       \"Community Area\": df['Community Area'].fillna(df['Community Area'].mode()[0])}\n",
        ")\n",
        "\n",
        "# Drop rows where Latitude/Longitude is missing (important for mapping)\n",
        "df = df.dropna(subset=['Latitude', 'Longitude'])\n",
        "\n",
        "# Verify that missing values are handled\n",
        "print(\"Missing Values After Cleaning:\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "2LFIR62OHgrq",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Convert Dates to Proper Format\n",
        "# Convert 'Date' column to datetime with specified format\n",
        "df['Date'] = pd.to_datetime(df['Date'], format=\"%Y-%m-%d %H:%M:%S\", errors='coerce')\n",
        "\n",
        "# Extract useful time-related features\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['DayOfWeek'] = df['Date'].dt.day_name()\n",
        "\n",
        "# Display changes\n",
        "print(\"Date column converted successfully. Sample:\")\n",
        "df[['Date', 'Year', 'Month', 'DayOfWeek']].head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "N6gsLyOBIFV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Standardize Text Data (Categorical Columns)\n",
        "# Convert text columns to lowercase for consistency\n",
        "df['Primary Type'] = df['Primary Type'].str.lower()\n",
        "df['Location Description'] = df['Location Description'].str.lower()\n",
        "\n",
        "# Verify changes\n",
        "print(\"Standardized Categorical Columns:\")\n",
        "print(df['Primary Type'].unique())\n",
        "\n",
        "# Check unique values in 'Description' column\n",
        "print(\"Theft Subcategories:\")\n",
        "print(df['Description'].unique())\n",
        "\n",
        "print(\"Unique Theft Descriptions:\")\n",
        "print(df['Description'].value_counts())"
      ],
      "metadata": {
        "id": "7jeH_Qq_JMYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Remove Duplicates\n",
        "# Check for duplicates\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"Found {duplicates} duplicate rows.\")\n",
        "\n",
        "# Remove duplicates\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Verify\n",
        "print(\"Data after removing duplicates:\", df.shape)"
      ],
      "metadata": {
        "id": "Qu6LBzHOJM-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Ensure Numerical Columns Are Correct\n",
        "# Ensure numeric columns are properly formatted\n",
        "numeric_cols = ['Ward', 'Community Area', 'Beat', 'District']\n",
        "df[numeric_cols] = df[numeric_cols].astype(int)\n",
        "\n",
        "# Check summary of numeric values\n",
        "print(\"Numeric Column Summary:\")\n",
        "print(df[numeric_cols].describe())"
      ],
      "metadata": {
        "id": "paYD6wSbJPdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display dataset info after cleaning\n",
        "print(\"Cleaned Dataset Info:\")\n",
        "print(df.info())\n",
        "\n",
        "# Display first few rows of the cleaned dataset\n",
        "df.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_TI39adYJR9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files  # Import the files module to enable file downloads in Colab\n",
        "\n",
        "# Ensure the file is saved before downloading\n",
        "df.to_csv(\"Cleaned_Retail_Theft.csv\", index=False)\n",
        "# Saves the cleaned DataFrame as a CSV file\n",
        "# 'index=False' ensures that Pandas does not write the index column in the CSV file\n",
        "\n",
        "# Download the file from Colab to your local system\n",
        "files.download(\"Cleaned_Retail_Theft.csv\")\n",
        "# This triggers a file download, allowing you to manually save the CSV file on your computer"
      ],
      "metadata": {
        "id": "uwoK7wxSN8nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Google Drive module\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access files stored in my Drive\n",
        "# This allows my to read and write files between Colab and Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MiKJftPhO452"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd  # For handling data (loading, cleaning, and analyzing)\n",
        "import matplotlib.pyplot as plt  # For plotting charts and visualizing data\n",
        "import folium  # For interactive map visualization\n",
        "from folium.plugins import HeatMap  # For creating heatmaps of crime locations\n",
        "import warnings  # For handling warnings\n",
        "\n",
        "# Turn off warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "2fe8yCC7Q5Ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "cleaned_retail_theft = \"/content/drive/MyDrive/PERSONAL_PROJECT/Retail_Theft_Analysis_Project/Cleaned_Retail_Theft.csv\"\n",
        "crt = pd.read_csv(cleaned_retail_theft)"
      ],
      "metadata": {
        "id": "ybxj22IfO_wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display dataset info\n",
        "print(\"Dataset Overview:\")\n",
        "print(crt.info())\n",
        "\n",
        "# Check first few rows\n",
        "crt.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8a9IeWVrPuBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(\"Missing Values:\")\n",
        "print(crt.isnull().sum())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6F6Pb0mxQ86c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary statistics\n",
        "print(\"Summary Statistics:\")\n",
        "print(crt.describe())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "V96wtDjzRrA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique values in key categorical columns\n",
        "print(\"Unique values in 'Primary Type' column (Crime Category):\")\n",
        "print(crt['Primary Type'].unique())\n",
        "\n",
        "print(\"\\n Unique values in 'Location Description' column:\")\n",
        "print(crt['Location Description'].unique())\n",
        "\n",
        "print(\"\\n Unique values in 'Arrest' column:\")\n",
        "print(crt['Arrest'].unique())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "v54y0n4mRwbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the last 5 rows of the dataset\n",
        "crt.tail()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "HvhgwF1UR54z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display all column names\n",
        "print(\"Column Names:\")\n",
        "print(crt.columns)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bhx7xb6TSRgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have explored the structure and distribution of our dataset, it’s time to dive deeper into Descriptive Analysis. While basic data exploration helped us understand missing values, column types, and general trends, Descriptive Analysis allows us to summarize, interpret, and extract meaningful insights from the data.\n",
        "\n",
        "Through Descriptive Statistics, we can:\n",
        "*   Identify patterns in retail theft incidents.\n",
        "*   Understand the distribution of key variables such as location, time, and arrest rates.\n",
        "*   Spot potential outliers or unusual behaviors.\n",
        "*   Gain insights into how theft trends vary across different neighborhoods and time periods.\n",
        "\n",
        "To achieve this, let’s first define Descriptive Analysis and its significance in data analysis."
      ],
      "metadata": {
        "id": "WdeQKZhATWIj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is Descriptive Analysis?** Descriptive Analysis is a statistical method used to summarize and interpret data in a meaningful way. It provides insights into the central tendency (mean, median, mode), dispersion (variance, standard deviation), and distribution of data points.\n",
        "\n",
        "💡 In simple terms: Descriptive Analysis helps us to understand what the data is telling us before applying deeper statistical or predictive techniques."
      ],
      "metadata": {
        "id": "bSCHQ8DgTyKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why is Descriptive Analysis Important?**\n",
        "\n",
        "**1) Identifies Trends –** Helps us recognize patterns in crime rates across different locations and time periods.\n",
        "\n",
        "**2) Summarizes Data Efficiently –** Provides key statistics like averages, percentages, and frequency counts for decision-making.\n",
        "\n",
        "**3) Detects Outliers –** Helps in spotting unusual spikes in crime rates or anomalies in theft patterns.\n",
        "\n",
        "**4) Supports Business & Policy Decisions –** Retailers, law enforcement, and policymakers can use these insights to improve crime prevention strategies.\n",
        "\n",
        "Now, let’s proceed with Descriptive Analysis to uncover more meaningful insights from our cleaned dataset."
      ],
      "metadata": {
        "id": "1vGjcLUaUGgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display dataset structure\n",
        "crt.info()\n",
        "\n",
        "# Show the first few rows\n",
        "crt.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vI3H4QfeU0-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Theft Frequency Analysis :-** Before diving into deep analysis, we need to understand the total number of incidents and how theft is distributed. This gives us a big picture of the problem."
      ],
      "metadata": {
        "id": "UcailmfXaU5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd  # For data handling and manipulation\n",
        "import matplotlib.pyplot as plt  # For data visualization\n",
        "import seaborn as sns  # For enhanced statistical plotting\n",
        "import warnings  # To handle unnecessary warnings\n",
        "\n",
        "# Turn off warnings for clean output\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"Libraries imported successfully! Warnings are turned off.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yTNQvl2lafa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display total number of theft incidents\n",
        "total_thefts = crt.shape[0]  # Count total rows\n",
        "print(f\"Total Retail Theft Incidents: {total_thefts}\")\n",
        "\n",
        "# Count theft cases based on 'Description' instead of 'Primary Type'\n",
        "theft_subcategories = crt['Description'].value_counts()\n",
        "\n",
        "# Display the top 5 theft subcategories\n",
        "print(\"\\n Top 5 Theft Subcategories:\")\n",
        "print(theft_subcategories.head())\n",
        "\n",
        "# Plot the top 5 theft subcategories\n",
        "plt.figure(figsize=(10,5))\n",
        "theft_subcategories.head(5).plot(kind='bar', color='blue')\n",
        "plt.title(\"Top 5 Most Common Theft Subcategories\")\n",
        "plt.xlabel(\"Theft Type\")\n",
        "plt.ylabel(\"Number of Incidents\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "h-popBQxdJhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:-** After analyzing the dataset, I found that all incidents fall under a single category—“Retail Theft”. This means that the dataset does not contain different types of theft, such as shoplifting, employee theft, or organized retail crime.\n",
        "\n",
        "Initially, I expected to see multiple theft categories, but after checking both the Primary Type and Description columns, it became clear that every record is labeled as Retail Theft. Because of this, breaking down theft into different subcategories is not possible with the given data.\n",
        "\n",
        "Since the dataset is focused entirely on Retail Theft, a better approach would be to analyze where these incidents happen the most, how often they lead to arrests, and whether there are any patterns over time.\n",
        "\n",
        "Now, let’s move on to Location-Based Analysis to understand which areas are most affected by retail theft."
      ],
      "metadata": {
        "id": "t1Fl243Kf2n3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Location-Based Analysis :-** Theft is not spread evenly across all locations. Some areas experience higher crime rates than others. Finding hotspots helps businesses and law enforcement focus on high-risk areas."
      ],
      "metadata": {
        "id": "mRj3OlK1f_ob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the most common locations where theft occurs\n",
        "location_counts = crt['Location Description'].value_counts()\n",
        "\n",
        "# Display the top 5 locations with the most theft cases\n",
        "print(\"\\n Top 5 Locations Where Theft Happens the Most:\")\n",
        "print(location_counts.head())\n",
        "\n",
        "# Plot the top 5 theft locations\n",
        "plt.figure(figsize=(10,5))\n",
        "location_counts.head(5).plot(kind='bar', color='red')\n",
        "plt.title(\"Top 5 Theft Locations\")\n",
        "plt.xlabel(\"Location Type\")\n",
        "plt.ylabel(\"Number of Incidents\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yn637nXdf-7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation :-** From the analysis, we found that department stores and small retail stores experience the highest number of retail theft incidents, followed by grocery food stores, drug stores, and convenience stores. This suggests that larger retail spaces and stores with high-value items are more vulnerable to theft.\n",
        "\n",
        "One key question that arises from this is: **Do theft incidents in these locations lead to arrests, or are offenders getting away without consequences?**\n",
        "\n",
        "To understand this better, let’s analyze the arrest data and see how frequently theft incidents result in an arrest. This will help us determine if law enforcement is effectively addressing retail theft and whether certain store types have a higher or lower likelihood of arrests."
      ],
      "metadata": {
        "id": "PAyDvNmkgNy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Arrest Analysis :-** Now here I understood where retail theft happens the most, the next important question is:\n",
        "Are these theft incidents leading to arrests, or are offenders getting away?\n",
        "\n",
        "Analyzing arrest data will help me:-\n",
        "\n",
        "✔ Understand how often retail theft cases result in an arrest.\n",
        "\n",
        "✔ Identify if certain locations or theft types have a higher arrest rate.\n",
        "\n",
        "✔ Determine how effective law enforcement is in handling retail theft.\n",
        "\n",
        "This will give us a clearer picture of whether theft hotspots also have high enforcement activity or if arrests are low despite high crime rates."
      ],
      "metadata": {
        "id": "tCd1V1zBgQ3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of theft incidents that led to arrests\n",
        "arrest_counts = crt['Arrest'].value_counts(normalize=True) * 100  # Convert to percentage\n",
        "\n",
        "# Display arrest statistics\n",
        "print(\"Arrest Rate Analysis:\")\n",
        "print(arrest_counts)\n",
        "\n",
        "# Plot the percentage of thefts leading to arrests\n",
        "plt.figure(figsize=(6,6))\n",
        "arrest_counts.plot(kind='pie', autopct='%1.1f%%', colors=['blue', 'orange'], labels=['No Arrest', 'Arrest'])\n",
        "plt.title(\"Percentage of Thefts Resulting in Arrests\")\n",
        "#plt.ylabel(\"\")  # Hide y-label for clarity\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qdt282PngXKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation :-** From the arrest analysis, I found that **51.9%** of retail theft incidents resulted in an arrest, while **48.1%** did not. This means that almost half of the reported thefts do not lead to any immediate legal action. While the arrest rate is fairly high, it still raises questions about why some cases don’t result in arrests—could it be due to lack of evidence, store policies, or law enforcement challenges?\n",
        "\n",
        "Another important aspect to consider is when theft incidents are happening the most. If we can identify high-theft periods (specific months, days, or times of the day), businesses and law enforcement can increase security and allocate resources more effectively.\n",
        "\n",
        "To explore this, let’s analyze time-based theft trends to see if theft cases spike on weekends, during holidays, or at specific times of the day."
      ],
      "metadata": {
        "id": "6LpsvGmRgf_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Time-Based Theft Trends :-** Understanding when theft incidents happen the most is just as important as knowing where they occur. This helps businesses and law enforcement:\n",
        "\n",
        "✔ Identify high-theft months (Are thefts higher during holidays or economic downturns?)\n",
        "\n",
        "✔ Find out if weekends have higher crime rates (Do people steal more on Saturdays and Sundays?)\n",
        "\n",
        "✔ Determine time-of-day patterns (Is theft more common in the morning, afternoon, or night?)\n",
        "\n",
        "By analyzing these trends, stores can increase security during high-risk periods, and law enforcement can allocate resources more efficiently.\n"
      ],
      "metadata": {
        "id": "GzNVzgjygi7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Theft Trends Per Month :-** Analyzing theft incidents by month helps us understand seasonal trends and identify high-risk periods when theft is more common. This allows businesses and law enforcement to increase security measures during peak months."
      ],
      "metadata": {
        "id": "6g2Sgbxmr4Rf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Theft Trends Per Month\n",
        "# Count number of theft incidents per month\n",
        "monthly_thefts = crt['Month'].value_counts().sort_index()\n",
        "\n",
        "# Display the theft count per month\n",
        "print(\"Theft Incidents Per Month:\")\n",
        "print(monthly_thefts)\n",
        "\n",
        "# Plot the number of thefts per month\n",
        "plt.figure(figsize=(10,5))\n",
        "monthly_thefts.plot(kind='bar', color='purple')\n",
        "plt.title(\"Retail Theft Incidents Per Month\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Number of Incidents\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SSbsshyYiVEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation :-** Theft is relatively consistent throughout the year, but peaks in July, August, October, and December.As December shows a higher number of incidents, possibly due to holiday shopping season, when stores are crowded, making theft easier. The lowest theft rates occur in February, likely due to seasonal factors. Since,now I know which months see the highest theft, the next step is to analyze theft trends by day of the week to see if certain weekdays are more prone to theft than others."
      ],
      "metadata": {
        "id": "isgLLRm8icGK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Theft Trends by Day of the Week :-** Understanding which days experience the most theft helps businesses and law enforcement strategically allocate resources and security staff on high-risk days."
      ],
      "metadata": {
        "id": "eWu7TKARie0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Theft Trends by Day of the Week\n",
        "# Count theft incidents per day of the week\n",
        "daywise_thefts = crt['DayOfWeek'].value_counts()\n",
        "\n",
        "# Display theft trends by day\n",
        "print(\"Theft Incidents Per Day of the Week:\")\n",
        "print(daywise_thefts)\n",
        "\n",
        "# Plot theft trends by day\n",
        "plt.figure(figsize=(10,5))\n",
        "daywise_thefts.plot(kind='bar', color='green')\n",
        "plt.title(\"Retail Theft Incidents Per Day of the Week\")\n",
        "plt.xlabel(\"Day of the Week\")\n",
        "plt.ylabel(\"Number of Incidents\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wul1lExzilqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation :-** Wednesday, Tuesday, and Friday have the highest theft incidents.Furthermore, Sunday has the lowest theft rate, which could be due to reduced store hours or fewer customers. Theft incidents remain fairly high on weekdays, likely because crowded stores make it easier for thieves to blend in. Since I know which days have the highest theft rates, the next step is to analyze what time of day theft is most frequent. This will help us determine if theft peaks during store opening hours, busy shopping times, or late at night."
      ],
      "metadata": {
        "id": "znpb7MEhis8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Theft Trends by Time of Day :-** Analyzing theft incidents by hour of the day helps identify when stores are most vulnerable to theft. This allows businesses to adjust security measures during peak hours to minimize losses.\n"
      ],
      "metadata": {
        "id": "pBdVs7HctCfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Theft Trends by Time of Day (if time data is available)\n",
        "# Convert 'Date' column to datetime format (if not already done)\n",
        "crt['Date'] = pd.to_datetime(crt['Date'])\n",
        "\n",
        "# Extract the hour from the Date column\n",
        "crt['Hour'] = crt['Date'].dt.hour\n",
        "\n",
        "# Count theft incidents by hour of the day\n",
        "hourly_thefts = crt['Hour'].value_counts().sort_index()\n",
        "\n",
        "# Display theft trends by time of day\n",
        "print(\"Theft Incidents Per Hour of the Day:\")\n",
        "print(hourly_thefts)\n",
        "\n",
        "# Plot theft trends by time of day\n",
        "plt.figure(figsize=(10,5))\n",
        "hourly_thefts.plot(kind='bar', color='orange')\n",
        "plt.title(\"Retail Theft Incidents Per Hour of the Day\")\n",
        "plt.xlabel(\"Hour of the Day\")\n",
        "plt.ylabel(\"Number of Incidents\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xsjr9muQqKw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation :-** Theft starts increasing from 9 AM and peaks between 2 PM and 4 PM. After 6 PM, theft incidents decline, with the lowest numbers between midnight and early morning (12 AM - 6 AM). The highest-risk period for retail theft is afternoon hours (12 PM - 5 PM).\n",
        "\n",
        "Now that we’ve identified when theft happens the most, the next important step is to analyze where theft is most concentrated geographically.\n",
        "\n",
        "While we know that certain months, days, and hours see higher theft rates, we now need to map out the locations where theft incidents are happening most frequently.\n",
        "\n",
        "By performing Geospatial Analysis (Mapping Theft Hotspots), we can:\n",
        "\n",
        "✔ Identify high-crime areas where theft is most common\n",
        "\n",
        "✔ Visualize theft hotspots to help law enforcement prioritize patrols\n",
        "\n",
        "✔ Assist businesses in enhancing security in high-risk locations\n",
        "\n",
        "Let’s move forward with Geospatial Analysis to uncover theft hotspots across Chicago!"
      ],
      "metadata": {
        "id": "XkFew6vItexx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Geospatial Analysis (Mapping Theft Hotspots) :-** Now that we understand when theft happens, we need to analyze where theft is most concentrated geographically. Mapping theft incidents helps:\n",
        "\n",
        "✔ Identify high-crime areas for targeted law enforcement actions.\n",
        "\n",
        "✔ Visualize theft hotspots so businesses can enhance security in high-risk locations.\n",
        "\n",
        "✔ Support policy decisions by focusing on regions with frequent theft incidents."
      ],
      "metadata": {
        "id": "BVqQF4MTiv_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import folium  # Used for creating interactive maps\n",
        "from folium.plugins import HeatMap  # Plugin to create a heatmap overlay\n",
        "import pandas as pd  # Used for handling data\n",
        "\n",
        "# Reload the dataset\n",
        "cleaned_retail_theft = \"/content/drive/MyDrive/PERSONAL_PROJECT/Retail_Theft_Analysis_Project/Cleaned_Retail_Theft.csv\"\n",
        "crt = pd.read_csv(cleaned_retail_theft)\n",
        "\n",
        "# Display the first few rows to check if Latitude and Longitude columns are present\n",
        "crt[['Latitude', 'Longitude']].head()\n",
        "\n",
        "# Create a base map centered on Chicago with a different map style for better clarity\n",
        "m = folium.Map(\n",
        "    location=[crt['Latitude'].mean(), crt['Longitude'].mean()],\n",
        "    zoom_start=11,\n",
        "    tiles=\"Stamen Toner\",  # Using Stamen Toner tiles\n",
        "    attr=\"Map tiles by Stamen Design, under CC BY 3.0. Data by OpenStreetMap, under ODbL.\"  # Attribution for Stamen Toner tiles\n",
        ")\n",
        "\n",
        "# Convert theft locations to a list for heatmap (Remove missing values)\n",
        "heat_data = crt[['Latitude', 'Longitude']].dropna().values.tolist()\n",
        "\n",
        "# **Apply Weighting Based on Theft Density (Optional)**\n",
        "heat_data_weighted = crt[['Latitude', 'Longitude']].dropna()\n",
        "heat_data_weighted['Weight'] = 1  # Assign equal weight (can be changed for advanced analysis)\n",
        "\n",
        "# Define a custom color gradient for better visualization\n",
        "gradient = {\n",
        "    0.2: \"blue\",    # Low theft density\n",
        "    0.4: \"green\",\n",
        "    0.6: \"yellow\",\n",
        "    0.8: \"orange\",\n",
        "    1.0: \"red\"      # High theft density\n",
        "}\n",
        "\n",
        "\n",
        "# Ensure all values in the gradient dictionary are strings\n",
        "gradient = {str(k): v for k, v in gradient.items()}\n",
        "\n",
        "# Add the heatmap with custom radius, opacity, and color gradient\n",
        "HeatMap(\n",
        "    heat_data_weighted[['Latitude', 'Longitude', 'Weight']].values,\n",
        "    radius=12,         # Adjust hotspot visibility\n",
        "    opacity=0.7,       # Adjust transparency for better readability\n",
        "    gradient=gradient  # Apply custom color scaling\n",
        ").add_to(m)\n",
        "\n",
        "# Save and display map\n",
        "m.save(\"theft_heatmap.html\")\n",
        "print(\"Heatmap saved as 'theft_heatmap.html'. Download and open in a browser to view.\")"
      ],
      "metadata": {
        "id": "u5fY3ZOZivpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.cluster import KMeans\n",
        "import folium\n",
        "from geopy.geocoders import Nominatim  # For reverse geocoding\n",
        "\n",
        "# Define number of clusters\n",
        "num_clusters = 5  # You can change this value depending on the number of clusters you want\n",
        "\n",
        "# Select only Latitude and Longitude columns for clustering\n",
        "theft_locations = crt[['Latitude', 'Longitude']]\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "crt['Cluster'] = kmeans.fit_predict(theft_locations)\n",
        "\n",
        "# Get cluster center locations\n",
        "cluster_centers = kmeans.cluster_centers_\n",
        "\n",
        "# Initialize geolocator for reverse geocoding with a unique user agent\n",
        "geolocator = Nominatim(user_agent=\"my-unique-user-agent\")  # Replace with a descriptive and unique name\n",
        "\n",
        "\n",
        "# Function to reverse geocode the center\n",
        "def get_location_name(latitude, longitude):\n",
        "    location = geolocator.reverse((latitude, longitude), language=\"en\")\n",
        "    return location.address if location else \"Unknown\"\n",
        "\n",
        "# Create a base map centered on Chicago\n",
        "cluster_map = folium.Map(location=[crt['Latitude'].mean(), crt['Longitude'].mean()], zoom_start=11)\n",
        "\n",
        "# Add clusters to the map\n",
        "for idx, row in crt.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[row['Latitude'], row['Longitude']],\n",
        "        radius=3,\n",
        "        color=[\"blue\", \"red\", \"green\", \"purple\", \"orange\"][row['Cluster']],  # Color each cluster differently\n",
        "        fill=True,\n",
        "        fill_color=[\"blue\", \"red\", \"green\", \"purple\", \"orange\"][row['Cluster']],\n",
        "        fill_opacity=0.6\n",
        "    ).add_to(cluster_map)\n",
        "\n",
        "# Add cluster centers to the map with location names\n",
        "for center in cluster_centers:\n",
        "    lat, lon = center\n",
        "    location_name = get_location_name(lat, lon)  # Get the location name for the cluster center\n",
        "    folium.Marker(\n",
        "        location=center,\n",
        "        popup=location_name,\n",
        "        icon=folium.Icon(color='black', icon=\"info-sign\")\n",
        "    ).add_to(cluster_map)\n",
        "\n",
        "# Display the map directly in the notebook\n",
        "cluster_map  # In Jupyter Notebook or Colab, this will display the map directly"
      ],
      "metadata": {
        "id": "uWKeUtykdJeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation :-** The heatmap visualization provides a clear view of theft hotspots across Chicago. This helps us understand where retail theft is most concentrated, allowing businesses and law enforcement to target high-risk areas more effectively. Now that I’ve identified where retail theft is most concentrated, the next important question is: What types of stores are being targeted the most? From the heatmap, I'd observed that theft is heavily clustered in high-traffic shopping areas such as downtown districts, major roads, and commercial zones. This raises an important consideration: Are certain store types more vulnerable to theft than others?\n",
        "\n",
        "✔ Are department stores facing more theft due to their large inventory and open layout?\n",
        "\n",
        "✔ Are small retail stores easy targets because of limited staff and security measures?\n",
        "\n",
        "✔ Do grocery stores and drug stores experience frequent theft of high-demand products (e.g., alcohol, medicine, or personal care items)?\n",
        "\n",
        "To explore this further, I'll analyze the impact of store type on retail theft, helping businesses understand which types of retail locations need stronger theft prevention measures.Let’s dive into the data!"
      ],
      "metadata": {
        "id": "_1pTqMXdjLVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Impact of Store Type on Theft :-** Now that I'd identified where theft is most concentrated, it’s important to understand which types of stores are targeted the most.\n",
        "\n",
        "By analyzing the impact of store type on theft, I can:\n",
        "\n",
        "✔ Identify which store categories (department stores, small retail stores, grocery stores, etc.) experience the highest theft incidents\n",
        "\n",
        "✔ Help retailers implement better security measures based on their store type\n",
        "\n",
        "✔ Assist law enforcement in prioritizing patrols in store types that face higher theft risks"
      ],
      "metadata": {
        "id": "Lv36JQ9BjPxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd  # For data handling and manipulation\n",
        "import matplotlib.pyplot as plt  # For data visualization\n",
        "import seaborn as sns  # For enhanced statistical plotting\n",
        "\n",
        "# Display the first few rows to check the 'Location Description' column\n",
        "crt[['Location Description']].head()\n",
        "\n",
        "# Count the number of theft incidents for each store type\n",
        "store_type_counts = crt['Location Description'].value_counts()\n",
        "\n",
        "# Display the top 10 store types with the highest theft incidents\n",
        "print(\"Top 10 Store Types Affected by Retail Theft:\")\n",
        "print(store_type_counts.head(10))\n",
        "\n",
        "# Plot the top 10 store types most affected by theft\n",
        "plt.figure(figsize=(12,6))\n",
        "store_type_counts.head(10).plot(kind='bar', color='orange')\n",
        "plt.title(\"Top 10 Store Types Affected by Retail Theft\")\n",
        "plt.xlabel(\"Store Type\")\n",
        "plt.ylabel(\"Number of Incidents\")\n",
        "plt.xticks(rotation=45)  # Rotate labels for better readability\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "roSw2TzhjVU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation :-** From the analysis, I found that department stores and small retail stores experience the highest number of theft incidents, followed by grocery stores, drug stores, and convenience stores. This suggests that larger retail spaces and stores with high-demand products are more vulnerable to theft.\n",
        "\n",
        "Some key takeaways from this analysis:\n",
        "\n",
        "✔ Department stores are the most targeted due to their large inventory and open floor plans, making it easier for shoplifters to steal unnoticed.\n",
        "\n",
        "✔ Small retail stores are also highly affected, possibly due to less staff supervision and weaker security systems.\n",
        "\n",
        "✔ Grocery stores and drug stores experience theft of everyday essentials, indicating that shoplifting may be driven by necessity or organized retail crime.\n",
        "\n",
        "Now, I know which store types are most vulnerable, the next logical step is to predict when and where theft is most likely to happen in the future.\n"
      ],
      "metadata": {
        "id": "i7Cn860ejcGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding past trends is useful, but to truly help businesses and law enforcement stay ahead of crime, we need to forecast future theft incidents.\n",
        "\n",
        "Now that I'd identified when, where, and which store types are most affected by theft, the next step is to predict future theft incidents using past trends and patterns.\n",
        "\n",
        "**Predictive analysis** allows businesses and law enforcement to anticipate crime before it happens, enabling proactive decision-making rather than reactive measures.\n",
        "\n",
        "Now that I’d explored past theft patterns, high-risk locations, store types, and law enforcement effectiveness, the next step is to predict future theft incidents using advanced machine learning techniques.\n",
        "\n",
        "To build an advanced machine learning and neural network model for predicting retail theft trends, we will focus on four key areas:\n",
        "\n",
        "1) Can we predict the exact day, month, or time of day when theft is most likely to occur?\n",
        "\n",
        "2)  Can we predict which areas in the city will experience more theft incidents in the future?\n",
        "\n",
        "3) Can we predict which store types (department stores, grocery stores, drug stores) will be more vulnerable to theft?\n",
        "\n",
        "4) Can we predict whether a reported theft will result in an arrest, based on time, location, and store type?"
      ],
      "metadata": {
        "id": "JIh6aYi42pg1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) Can we predict the exact day, month, or time of day when theft is most likely to occur?**\n",
        "\n",
        "Now that I'd explored past theft trends, the next step is to predict when theft incidents are most likely to occur in the future.\n",
        "\n",
        "Retail theft is not random—it follows patterns based on time. By analyzing past data, I can identify:\n",
        "\n",
        "✔ Whether theft increases during specific months (holidays, summer, etc.)\n",
        "\n",
        "✔ If certain days of the week experience more theft than others\n",
        "\n",
        "✔ What time of the day stores are most vulnerable\n",
        "\n",
        "**Purpose of This Analysis :-** By forecasting theft incidents, businesses and law enforcement can:\n",
        "\n",
        "✔ Prepare in advance for high-theft periods\n",
        "\n",
        "✔ Allocate police resources efficiently based on predicted crime surges\n",
        "\n",
        "✔ Help retailers strengthen security during expected theft spikes\n",
        "\n",
        "This allows decision-makers to be proactive rather than reactive, reducing theft incidents before they happen.\n",
        "\n",
        "**Why Use Time-Series Forecasting for This?** Time-series forecasting helps us predict future theft trends based on historical data. It is useful because:\n",
        "\n",
        "✔ Theft incidents follow seasonal and weekly patterns that can be modeled mathematically.\n",
        "\n",
        "✔ It allows us to forecast theft rates for future months and days.\n",
        "\n",
        "✔ Businesses can use these forecasts to optimize staffing, security, and inventory control.\n",
        "\n",
        "To predict when theft is most likely to happen, we will use three powerful forecasting models:\n",
        "\n",
        "**1) ARIMA – A statistical model that captures time-dependent patterns.**\n",
        "\n",
        "**2) Facebook Prophet – A machine-learning model designed for trend detection.**\n",
        "\n",
        "**3) LSTM (Neural Networks) – A deep learning model for identifying complex theft trends.**\n",
        "\n",
        "Each model has its own advantages, and we will compare their results to see which one provides the best predictions."
      ],
      "metadata": {
        "id": "RIgCquEvHP7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ARIMA Model (Auto-Regressive Integrated Moving Average) :-** ARIMA is a traditional statistical forecasting model that analyzes past values and trends to predict future outcomes. It is widely used for time-series data where values are dependent on past observations.\n",
        "\n",
        "**Why Use ARIMA for Retail Theft Prediction?**\n",
        "\n",
        "✔ ARIMA is simple but effective for detecting trends in historical theft data.\n",
        "\n",
        "✔ It is useful when data follows a linear trend without major seasonal effects.\n",
        "\n",
        "✔ It provides a quick and interpretable forecasting approach."
      ],
      "metadata": {
        "id": "SFMZXuesOrVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd  # For handling data\n",
        "import matplotlib.pyplot as plt  # For visualization\n",
        "import warnings  # To ignore unnecessary warnings\n",
        "from statsmodels.tsa.arima.model import ARIMA  # ARIMA model for forecasting\n",
        "\n",
        "# Turn off warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "DjljFBfFP8LT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-Load the dataset\n",
        "cleaned_retail_theft = \"/content/drive/MyDrive/PERSONAL_PROJECT/Retail_Theft_Analysis_Project/Cleaned_Retail_Theft.csv\"\n",
        "crt = pd.read_csv(cleaned_retail_theft)"
      ],
      "metadata": {
        "id": "pMB04zMyPxjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Date' column to datetime format\n",
        "crt['Date'] = pd.to_datetime(crt['Date'])\n",
        "\n",
        "# Set 'Date' as index\n",
        "crt.set_index('Date', inplace=True)\n",
        "\n",
        "# Resample data to get the number of theft incidents per month\n",
        "crt_monthly = crt.resample('M').size()\n",
        "\n",
        "# Remove early low-count years (keep data from 2003 onward)\n",
        "crt_monthly = crt_monthly[crt_monthly.index >= \"2003-01-01\"]\n",
        "\n",
        "# Check for missing values in the dataset\n",
        "missing_values = crt_monthly.isnull().sum()\n",
        "print(f\"Missing Values in Data: {missing_values}\")\n",
        "\n",
        "# Fill missing values with the mean theft count for that month\n",
        "crt_monthly.fillna(crt_monthly.mean(), inplace=True)\n",
        "\n",
        "# Display the first few rows after cleaning\n",
        "print(\"Updated Monthly Theft Counts:\\n\", crt_monthly.head())\n",
        "\n",
        "# Plot the cleaned dataset\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(crt_monthly, label=\"Theft Incidents\")\n",
        "plt.title(\"Monthly Theft Incidents Over Time (Cleaned Data)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Number of Thefts\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "d9KXiv8l8uXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define ARIMA model parameters (p,d,q) based on historical trends\n",
        "arima_model = ARIMA(crt_monthly, order=(5,1,0))  # Example (p=5, d=1, q=0) values\n",
        "\n",
        "# Fit the model to the dataset\n",
        "arima_result = arima_model.fit()\n",
        "\n",
        "# Forecast theft incidents for the next 12 months\n",
        "arima_forecast = arima_result.forecast(steps=12)\n",
        "\n",
        "# Plot the historical data and ARIMA forecast\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(crt_monthly, label=\"Historical Theft Data\")  # Plot past theft trends\n",
        "plt.plot(pd.date_range(crt_monthly.index[-1], periods=12, freq='M'), arima_forecast, label=\"ARIMA Forecast\", color=\"red\")\n",
        "plt.title(\"Retail Theft Forecast using ARIMA Model (After Cleaning)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Number of Thefts\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_3eVYgIaRp-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation :-** From the ARIMA model, I'd observed that theft incidents have fluctuated significantly over time, with periods of increase and decline. The model successfully captures the general trend, but there are some important takeaways :-\n",
        "\n",
        "✔ ARIMA captures long-term trends but struggles with seasonality (holiday spikes, weekend fluctuations, etc.).\n",
        "\n",
        "✔ The forecasted values are relatively stable, meaning the model assumes theft will continue at a similar rate without strong fluctuations.\n",
        "\n",
        "✔ While ARIMA is good for general trend prediction, it does not automatically adjust for seasonal effects like holiday shopping seasons, economic changes, or law enforcement actions.\n",
        "\n",
        "Since retail theft is affected by seasonal patterns (e.g., higher theft during the holidays, weekends, or economic downturns), we need a model that can:-\n",
        "\n",
        "✔ Automatically detect seasonality and trends without manual adjustments.\n",
        "\n",
        "✔ Handle irregular patterns in theft incidents, such as crime spikes.\n",
        "\n",
        "✔ Provide better short-term predictions based on real-world patterns.\n"
      ],
      "metadata": {
        "id": "cb-_IdqaUwYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Facebook Prophet Analysis :-** Facebook Prophet is an open-source time-series forecasting model developed by Meta (formerly Facebook). It is designed to automatically detect trends, seasonality, and outliers in time-series data, making it useful for business analytics, financial forecasting, and crime trend analysis.\n",
        "\n",
        "Unlike ARIMA, Prophet:\n",
        "\n",
        "✔ Handles missing data and outliers better.\n",
        "\n",
        "✔ Automatically detects seasonality (daily, weekly, yearly patterns).\n",
        "\n",
        "✔ Adapts to sudden trend changes without manual intervention.\n",
        "\n",
        "**Why Use Facebook Prophet for Retail Theft Prediction?** Retail theft is not random—it follows seasonal trends and periodic fluctuations based on factors like:\n",
        "\n",
        "✔ Holidays and shopping seasons (Black Friday, Christmas, back-to-school sales)\n",
        "\n",
        "✔ Weekends vs. weekdays (Stores are busier on weekends)\n",
        "\n",
        "✔ Economic conditions (Theft might increase during financial crises)\n",
        "\n",
        "Since ARIMA struggles with complex seasonality, Facebook Prophet helps by:\n",
        "\n",
        "✔ Automatically identifying seasonal patterns in theft incidents.\n",
        "\n",
        "✔ Handling irregular crime spikes better than traditional models.\n",
        "\n",
        "✔ Providing flexible and interpretable forecasts for businesses and law enforcement."
      ],
      "metadata": {
        "id": "6xUby9fZVTKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install prophet"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_7DAveqTM5bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd  # For data handling\n",
        "import matplotlib.pyplot as plt  # For visualization\n",
        "import warnings  # To ignore unnecessary warnings\n",
        "from prophet import Prophet  # Facebook Prophet model for time-series forecasting\n",
        "\n",
        "# Turn off warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "metadata": {
        "id": "5MBvNkTFMUFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the cleaned dataset\n",
        "cleaned_retail_theft = \"/content/drive/MyDrive/PERSONAL_PROJECT/Retail_Theft_Analysis_Project/Cleaned_Retail_Theft.csv\"\n",
        "crt = pd.read_csv(cleaned_retail_theft)"
      ],
      "metadata": {
        "id": "YvndLbKlMZGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Date' column to datetime format\n",
        "crt['Date'] = pd.to_datetime(crt['Date'])\n",
        "\n",
        "# Set 'Date' as index for time-series analysis\n",
        "crt.set_index('Date', inplace=True)\n",
        "\n",
        "# Resample data to get the number of theft incidents per month\n",
        "crt_monthly = crt.resample('M').size()\n",
        "\n",
        "# Remove early low-count years (keep data from 2003 onward)\n",
        "crt_monthly = crt_monthly[crt_monthly.index >= \"2003-01-01\"]\n",
        "\n",
        "# Prepare data for Prophet (Prophet requires specific column names)\n",
        "prophet_data = crt_monthly.reset_index()  # Reset index to keep Date as a column\n",
        "prophet_data.columns = ['ds', 'y']  # Prophet requires 'ds' (date) and 'y' (value)\n",
        "\n",
        "# Display the first few rows to confirm format\n",
        "print(\"Prepared Data for Prophet:\\n\", prophet_data.head())\n",
        "\n",
        "# Define and train the Prophet model\n",
        "prophet_model = Prophet()  # Initialize Prophet model\n",
        "prophet_model.fit(prophet_data)  # Train the model on theft data\n",
        "\n",
        "# Create a future dataframe for predictions (Next 12 months)\n",
        "future = prophet_model.make_future_dataframe(periods=12, freq='M')\n",
        "\n",
        "# Predict future theft incidents\n",
        "forecast = prophet_model.predict(future)\n",
        "\n",
        "# Display first few rows of forecasted values\n",
        "print(\"Future Forecast Data:\\n\", forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail())\n",
        "\n",
        "# Plot Prophet forecast results\n",
        "plt.figure(figsize=(12,6))\n",
        "prophet_model.plot(forecast)\n",
        "plt.title(\"Retail Theft Forecast using Facebook Prophet\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Number of Thefts\")\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SLg3TS8SNiAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation :-** From the Facebook Prophet forecast, I found that theft incidents follow strong seasonal patterns and periodic fluctuations, making it more effective than ARIMA in detecting short-term variations. However, while Prophet successfully captures seasonality, it still assumes a structured trend and does not fully account for hidden relationships and sudden spikes in theft incidents. The widening confidence intervals suggest that external factors, such as law enforcement policies, economic conditions, and retail security measures, impact crime trends in ways Prophet cannot explicitly model. To address these limitations, we will implement LSTM (Long Short-Term Memory Neural Networks), which can learn from historical crime patterns without predefined trends, capture complex dependencies, and adapt to real-time fluctuations in theft incidents. By comparing ARIMA, Prophet, and LSTM, we aim to determine which model provides the most accurate and reliable theft predictions"
      ],
      "metadata": {
        "id": "B3-zGx-KaybO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM (Long Short-Term Memory) for Retail Theft Forecasting :-** LSTM (Long Short-Term Memory) is a type of neural network specifically designed for time-series forecasting. Unlike traditional models like ARIMA and Facebook Prophet, LSTM can:\n",
        "\n",
        "✔ Capture long-term dependencies in data, meaning it remembers past patterns over a longer period.\n",
        "\n",
        "✔ Handle non-linear trends that traditional models might miss.\n",
        "\n",
        "✔ Learn from past fluctuations in theft incidents to make accurate future predictions.\n",
        "\n",
        "**Why Use LSTM for Retail Theft Prediction?** Retail theft does not always follow a simple trend—it is influenced by multiple external factors such as:\n",
        "\n",
        "✔ Economic downturns (more financial stress, higher theft rates)\n",
        "\n",
        "✔ Law enforcement changes (more patrols could decrease theft)\n",
        "\n",
        "✔ Retail security policies (improved store security reduces theft incidents)\n",
        "\n",
        "Since LSTM is a deep learning model, it can:\n",
        "\n",
        "✔ Detect complex crime patterns that other models may miss\n",
        "\n",
        "✔ Handle irregular spikes in theft incidents more effectively\n",
        "\n",
        "✔ Predict theft trends based on past behaviors and unseen patterns\n",
        "\n",
        "This makes LSTM a powerful forecasting tool for predicting when theft will occur with higher accuracy compared to ARIMA and Prophet.\n"
      ],
      "metadata": {
        "id": "aT-3ykaFIp4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Google Drive module\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access files stored in my Drive\n",
        "# This allows my to read and write files between Colab and Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "JJ4Y1mdxJIE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd  # For data handling\n",
        "import numpy as np  # For numerical operations\n",
        "import matplotlib.pyplot as plt  # For visualization\n",
        "import warnings  # To ignore unnecessary warnings\n",
        "from sklearn.preprocessing import MinMaxScaler  # To normalize data for LSTM\n",
        "import tensorflow as tf  # For building the LSTM model\n",
        "from tensorflow.keras.models import Sequential  # To create the LSTM model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout  # LSTM layers\n",
        "\n",
        "# Turn off warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "MhHiJI5PJC4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the cleaned dataset\n",
        "cleaned_retail_theft = \"/content/drive/MyDrive/PERSONAL_PROJECT/Retail_Theft_Analysis_Project/Cleaned_Retail_Theft.csv\"\n",
        "crt = pd.read_csv(cleaned_retail_theft)\n",
        "\n",
        "# Convert 'Date' column to datetime format\n",
        "crt['Date'] = pd.to_datetime(crt['Date'])\n",
        "\n",
        "# Set 'Date' as index for time-series analysis\n",
        "crt.set_index('Date', inplace=True)\n",
        "\n",
        "# Resample data to get the number of theft incidents per month\n",
        "crt_monthly = crt.resample('M').size()\n",
        "\n",
        "# Remove early low-count years (keep data from 2003 onward)\n",
        "crt_monthly = crt_monthly[crt_monthly.index >= \"2003-01-01\"]\n",
        "\n",
        "# Normalize the data before feeding it into LSTM (values between 0 and 1)\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "crt_scaled = scaler.fit_transform(crt_monthly.values.reshape(-1,1))\n",
        "\n",
        "# Display the first few rows\n",
        "print(\"Scaled Theft Data:\\n\", crt_scaled[:5])"
      ],
      "metadata": {
        "id": "6T_RU8H4JOHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare training data for LSTM\n",
        "X, y = [], []\n",
        "time_step = 12  # Using past 12 months to predict the next month\n",
        "\n",
        "for i in range(len(crt_scaled) - time_step - 1):\n",
        "    X.append(crt_scaled[i:(i+time_step), 0])\n",
        "    y.append(crt_scaled[i + time_step, 0])\n",
        "\n",
        "# Convert lists into NumPy arrays\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "# Reshape input data to be compatible with LSTM model\n",
        "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "# Display shape of training data\n",
        "print(f\"Training Data Shape: X={X.shape}, y={y.shape}\")"
      ],
      "metadata": {
        "id": "ty1jniqwJroR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import additional library for saving model architecture\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(50, return_sequences=True, input_shape=(time_step, 1)),  # First LSTM layer\n",
        "    Dropout(0.2),  # Dropout to prevent overfitting\n",
        "    LSTM(50, return_sequences=False),  # Second LSTM layer\n",
        "    Dense(25),  # Additional dense layer\n",
        "    Dense(1)  # Output layer for predicting theft values\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the LSTM model\n",
        "model.fit(X, y, epochs=50, batch_size=16, verbose=1)\n",
        "\n",
        "# Save the model architecture as an image\n",
        "plot_model(model, to_file=\"lstm_model_architecture.png\", show_shapes=True, show_layer_names=True)\n",
        "\n",
        "print(\"LSTM model architecture saved as 'lstm_model_architecture.png'.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "U4IbCc_MJu5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained LSTM model (for future use)\n",
        "model.save(\"lstm_theft_forecast.h5\")\n",
        "print(\"LSTM trained model saved as 'lstm_theft_forecast.h5'.\")"
      ],
      "metadata": {
        "id": "Mp-ZmJ_2LDH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained LSTM model\n",
        "model = tf.keras.models.load_model(\"lstm_theft_forecast.h5\")\n",
        "\n",
        "# Forecast future theft incidents using the trained model\n",
        "future_inputs = crt_scaled[-time_step:].reshape(1, time_step, 1)  # Ensure proper shape\n",
        "lstm_forecast = []\n",
        "\n",
        "for _ in range(12):  # Predict next 12 months\n",
        "    pred = model.predict(future_inputs)  # Make prediction\n",
        "    lstm_forecast.append(pred[0, 0])  # Store the predicted value\n",
        "\n",
        "    # Correctly reshape `pred` before appending\n",
        "    pred_reshaped = np.reshape(pred[0,0], (1,1,1))  # Convert to 3D shape\n",
        "\n",
        "    # Append new prediction to future_inputs (keeping the shape consistent)\n",
        "    future_inputs = np.concatenate((future_inputs[:, 1:, :], pred_reshaped), axis=1)\n",
        "\n",
        "# Convert forecasted values back to original scale\n",
        "lstm_forecast = scaler.inverse_transform(np.array(lstm_forecast).reshape(-1,1))\n",
        "\n",
        "# Display forecasted values\n",
        "print(\"LSTM Forecast for Next 12 Months:\\n\", lstm_forecast)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "uhnabbveKiGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot LSTM forecast results\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(crt_monthly, label=\"Historical Theft Data\")\n",
        "plt.plot(pd.date_range(crt_monthly.index[-1], periods=12, freq='M'), lstm_forecast, label=\"LSTM Forecast\", color=\"green\")\n",
        "plt.title(\"Retail Theft Forecast using LSTM Neural Network\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Number of Thefts\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0W8ZJizPLrVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation :-** From the LSTM forecast, we can see that the model effectively captures long-term dependencies in theft trends and provides a stable prediction for the next 12 months. Unlike ARIMA, which produced a relatively flat forecast, LSTM identifies subtle patterns in theft incidents and adjusts predictions accordingly. Compared to Facebook Prophet, which captured strong seasonal variations, LSTM forecasts are smoother and less reactive to short-term fluctuations. While this makes LSTM useful for general trend forecasting, it may not fully account for seasonal spikes in theft, such as those observed during holiday seasons or economic downturns. Additionally, the LSTM model structure, consisting of multiple layers and dropout mechanisms, ensures that it can learn long-term theft behaviors while reducing overfitting. However, the forecast suggests a gradual stabilization of theft incidents, which differs slightly from the seasonal fluctuations seen in Prophet’s results.\n",
        "\n",
        "Now that we have three forecasting models—ARIMA, Facebook Prophet, and LSTM—each providing different insights, it’s important to compare their accuracy, trend detection, and seasonality handling. By visualizing all three models together and calculating their performance metrics (such as RMSE and MAE), we can determine which model provides the most accurate and useful predictions for retail theft incidents. This final comparison will help in identifying the best approach for real-world applications, such as predicting high-risk months for theft, optimizing store security measures, and aiding law enforcement in resource allocation.\n",
        "\n",
        "Now that we have completed all three forecasting models, let’s compare their performance side by side to determine the best model for predicting retail theft incidents."
      ],
      "metadata": {
        "id": "EsfB-6JyQZm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot All Three Forecasts Together\n",
        "# Import necessary libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define time range for the forecasted values (next 12 months)\n",
        "future_dates = pd.date_range(crt_monthly.index[-1], periods=12, freq='M')\n",
        "\n",
        "# Plot historical data\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(crt_monthly, label=\"Historical Theft Data\", color=\"black\")\n",
        "\n",
        "# Plot ARIMA forecast\n",
        "plt.plot(future_dates, arima_forecast, label=\"ARIMA Forecast\", color=\"red\")\n",
        "\n",
        "# Plot Facebook Prophet forecast\n",
        "plt.plot(forecast['ds'].tail(12), forecast['yhat'].tail(12), label=\"Facebook Prophet Forecast\", color=\"blue\")\n",
        "\n",
        "# Plot LSTM forecast\n",
        "plt.plot(future_dates, lstm_forecast, label=\"LSTM Forecast\", color=\"green\")\n",
        "\n",
        "# Formatting the chart\n",
        "plt.title(\"Retail Theft Forecast Comparison: ARIMA vs. Facebook Prophet vs. LSTM\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Number of Thefts\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WUgDZjItSari"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required metrics from sklearn to evaluate model performance\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Compute accuracy metrics for ARIMA\n",
        "# Mean Absolute Error (MAE) calculates the average absolute error between actual and predicted values.\n",
        "# Root Mean Squared Error (RMSE) measures the square root of the average squared differences between actual and predicted values.\n",
        "arima_mae = mean_absolute_error(crt_monthly[-12:], arima_forecast)\n",
        "arima_rmse = np.sqrt(mean_squared_error(crt_monthly[-12:], arima_forecast))\n",
        "\n",
        "# Compute accuracy metrics for Facebook Prophet\n",
        "# Prophet's forecasted values ('yhat') are extracted for the last 12 months to compare with actual data.\n",
        "prophet_mae = mean_absolute_error(crt_monthly[-12:], forecast['yhat'].tail(12))\n",
        "prophet_rmse = np.sqrt(mean_squared_error(crt_monthly[-12:], forecast['yhat'].tail(12)))\n",
        "\n",
        "# Compute accuracy metrics for LSTM\n",
        "# Since LSTM outputs an array, we compare it directly with actual values.\n",
        "lstm_mae = mean_absolute_error(crt_monthly[-12:], lstm_forecast)\n",
        "lstm_rmse = np.sqrt(mean_squared_error(crt_monthly[-12:], lstm_forecast))\n",
        "\n",
        "# Display results for all models\n",
        "# MAE and RMSE values are printed to compare model performance.\n",
        "# Lower MAE and RMSE values indicate better accuracy.\n",
        "print(f\"ARIMA: MAE = {arima_mae:.2f}, RMSE = {arima_rmse:.2f}\")\n",
        "print(f\"Facebook Prophet: MAE = {prophet_mae:.2f}, RMSE = {prophet_rmse:.2f}\")\n",
        "print(f\"LSTM: MAE = {lstm_mae:.2f}, RMSE = {lstm_rmse:.2f}\")"
      ],
      "metadata": {
        "id": "QCpYLYbPMwIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation :-** From my comparison, I found that Facebook Prophet performed the best in capturing seasonal trends, while LSTM was effective in learning hidden crime patterns, and ARIMA provided a stable but less responsive forecast. This highlights how theft incidents are influenced by recurring patterns such as holidays, economic conditions, and store traffic, making it possible to anticipate future spikes in theft. However, while time-based forecasting helps us understand when theft is likely to happen, the next critical question is where theft will occur. By analyzing location-based patterns, we can predict which areas in the city are most vulnerable to retail theft, helping law enforcement and businesses take preventive measures.\n"
      ],
      "metadata": {
        "id": "c-KUAbL9PUGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Can we predict which areas in the city will experience more theft incidents in the future?**\n",
        "Identifying areas with a high risk of future theft incidents helps law enforcement, retail stores, and policymakers take preventive measures. By understanding which locations are more vulnerable, we can:\n",
        "\n",
        "✔ Improve police patrolling strategies in high-crime areas.\n",
        "\n",
        "✔ Help businesses strengthen security in theft-prone locations.\n",
        "\n",
        "✔ Support city planners in designing safer urban spaces.\n",
        "\n",
        "What Type of Analysis Can I Do?\n",
        "\n",
        "To predict theft hotspots, we can use:\n",
        "\n",
        "**✔ Geospatial Analysis (Heatmaps & Clustering) →** Identify theft hotspots based on past incidents.\n",
        "\n",
        "**✔ Machine Learning (Classification & Regression Models) →** Predict future high-risk locations using crime trends.\n",
        "\n",
        "**✔ Time-Series & Seasonal Analysis →** Analyze if theft increases in specific areas over time."
      ],
      "metadata": {
        "id": "9lu7YeqwRW7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Geospatial Analysis (Heatmaps & Clustering) :-** I will first visualize past theft hotspots using a heatmap, then apply clustering techniques to identify high-risk areas."
      ],
      "metadata": {
        "id": "h4roBGadTM7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Google Drive module\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access files stored in my Drive\n",
        "# This allows my to read and write files between Colab and Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yiRZKjtjTgeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd  # For data handling\n",
        "import folium  # For interactive maps\n",
        "from folium.plugins import HeatMap  # For heatmap visualization\n",
        "import matplotlib.pyplot as plt  # For visualization\n",
        "import geopandas as gpd  # For spatial analysis\n",
        "from sklearn.cluster import KMeans  # For clustering high-risk areas\n",
        "import warnings  # To suppress warnings\n",
        "\n",
        "# Turn off warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "-Q9XP6CWTMhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the cleaned dataset\n",
        "cleaned_retail_theft = \"/content/drive/MyDrive/PERSONAL_PROJECT/Retail_Theft_Analysis_Project/Cleaned_Retail_Theft.csv\"\n",
        "crt = pd.read_csv(cleaned_retail_theft)\n",
        "\n",
        "# Convert 'Date' column to datetime format\n",
        "crt['Date'] = pd.to_datetime(crt['Date'])\n",
        "\n",
        "# Remove any missing values in location columns\n",
        "crt = crt.dropna(subset=['Latitude', 'Longitude'])\n",
        "\n",
        "# Display dataset structure\n",
        "print(\"Dataset Structure After Cleaning:\")\n",
        "print(crt.info())\n",
        "\n",
        "# Display first few rows to check data\n",
        "print(\"Sample Data:\")\n",
        "print(crt[['Date', 'Primary Type', 'Location Description', 'Latitude', 'Longitude']].head())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bVTVFGkMS_jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "import pandas as pd\n",
        "\n",
        "# Create a base map centered on Chicago\n",
        "map_chicago = folium.Map(location=[crt['Latitude'].mean(), crt['Longitude'].mean()], zoom_start=11)\n",
        "\n",
        "# Prepare data for the heatmap (Latitude, Longitude)\n",
        "heat_data = crt[['Latitude', 'Longitude']].dropna().values.tolist()\n",
        "\n",
        "# Add heatmap layer to the map\n",
        "HeatMap(heat_data, radius=10).add_to(map_chicago)\n",
        "\n",
        "# Display the heatmap in output\n",
        "map_chicago  # In Jupyter Notebook or Colab, this will display the map directly"
      ],
      "metadata": {
        "collapsed": true,
        "id": "B5XyqQhETt0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import folium\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Define number of clusters\n",
        "num_clusters = 5\n",
        "\n",
        "# Select only Latitude and Longitude columns for clustering\n",
        "theft_locations = crt[['Latitude', 'Longitude']]\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "crt['Cluster'] = kmeans.fit_predict(theft_locations)\n",
        "\n",
        "# Get cluster center locations\n",
        "cluster_centers = kmeans.cluster_centers_\n",
        "\n",
        "# Create a base map centered on Chicago\n",
        "cluster_map = folium.Map(location=[crt['Latitude'].mean(), crt['Longitude'].mean()], zoom_start=11)\n",
        "\n",
        "# Add clusters to the map\n",
        "for idx, row in crt.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[row['Latitude'], row['Longitude']],\n",
        "        radius=3,\n",
        "        color=[\"blue\", \"red\", \"green\", \"purple\", \"orange\"][row['Cluster']],\n",
        "        fill=True,\n",
        "        fill_color=[\"blue\", \"red\", \"green\", \"purple\", \"orange\"][row['Cluster']],\n",
        "        fill_opacity=0.6\n",
        "    ).add_to(cluster_map)\n",
        "\n",
        "# Add cluster centers\n",
        "for center in cluster_centers:\n",
        "    folium.Marker(location=center, icon=folium.Icon(color='black', icon=\"info-sign\")).add_to(cluster_map)\n",
        "\n",
        "# Display the map directly in the notebook\n",
        "cluster_map"
      ],
      "metadata": {
        "id": "H99UPBzzXzcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plot of clustered locations\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.scatter(crt['Longitude'], crt['Latitude'], c=crt['Cluster'], cmap='viridis', alpha=0.6)\n",
        "plt.scatter(cluster_centers[:,1], cluster_centers[:,0], c='red', marker='X', s=200, label=\"Cluster Centers\")\n",
        "plt.title(\"Clustered High-Risk Theft Areas\")\n",
        "plt.xlabel(\"Longitude\")\n",
        "plt.ylabel(\"Latitude\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MNfFDc4cZ_UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation :-** From my analysis, I'd identified five major theft-prone clusters across the city, with high-risk areas concentrated in downtown and commercial zones. The clusters revealed that some locations experience densely packed theft incidents, indicating hotspots with frequent crimes, while others are more spread out, suggesting theft occurs over a larger area. By mapping these patterns, we can help law enforcement focus patrol efforts, assist businesses in strengthening security, and support policymakers in crime prevention strategies.\n",
        "\n",
        "Now, to take this a step further,I can use **Machine Learning models** to predict future high-risk areas based on past crime trends. Let’s move on to classification and regression models to forecast where theft is most likely to happen next."
      ],
      "metadata": {
        "id": "ebN7QC_jdalo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Google Drive module\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access files stored in my Drive\n",
        "# This allows my to read and write files between Colab and Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "96qraeEfemo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd  # For data handling\n",
        "import numpy as np  # For numerical computations\n",
        "import matplotlib.pyplot as plt  # For visualizations\n",
        "import seaborn as sns  # For heatmaps and advanced plots\n",
        "from sklearn.model_selection import train_test_split  # For splitting dataset\n",
        "from sklearn.preprocessing import StandardScaler  # For normalizing data\n",
        "from sklearn.ensemble import RandomForestClassifier  # For classification model\n",
        "from sklearn.linear_model import LogisticRegression  # For binary classification\n",
        "from sklearn.ensemble import RandomForestRegressor  # For regression model\n",
        "from sklearn.metrics import accuracy_score, classification_report, mean_absolute_error, mean_squared_error  # For evaluating models\n",
        "import warnings  # To suppress warnings\n",
        "\n",
        "# Turn off warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "J3ia-pkEejZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the cleaned dataset\n",
        "cleaned_retail_theft = \"/content/drive/MyDrive/PERSONAL_PROJECT/Retail_Theft_Analysis_Project/Cleaned_Retail_Theft.csv\"\n",
        "crt = pd.read_csv(cleaned_retail_theft)\n",
        "\n",
        "# Convert 'Date' column to datetime format\n",
        "crt['Date'] = pd.to_datetime(crt['Date'])\n",
        "\n",
        "# Extract useful time-based features\n",
        "crt['Year'] = crt['Date'].dt.year\n",
        "crt['Month'] = crt['Date'].dt.month\n",
        "crt['DayOfWeek'] = crt['Date'].dt.dayofweek  # Monday=0, Sunday=6\n",
        "\n",
        "# Remove missing values in location columns\n",
        "crt = crt.dropna(subset=['Latitude', 'Longitude'])\n",
        "\n",
        "# Display dataset structure after feature extraction\n",
        "print(\"Updated Dataset Structure:\")\n",
        "print(crt.info())\n",
        "\n",
        "# Display first few rows to confirm changes\n",
        "print(\"Sample Data:\")\n",
        "print(crt[['Year', 'Month', 'DayOfWeek', 'Latitude', 'Longitude', 'Primary Type']].head())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "boI3wln5fXxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd  # For data handling\n",
        "import numpy as np  # For numerical operations\n",
        "from sklearn.preprocessing import StandardScaler  # Importing StandardScaler for normalization\n",
        "from sklearn.model_selection import train_test_split  # FIX: Importing train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier  # Importing Random Forest for classification\n",
        "from sklearn.metrics import accuracy_score, classification_report  # Importing evaluation metrics\n",
        "\n",
        "# Load the cleaned dataset\n",
        "cleaned_retail_theft = \"/content/drive/MyDrive/PERSONAL_PROJECT/Retail_Theft_Analysis_Project/Cleaned_Retail_Theft.csv\"\n",
        "crt = pd.read_csv(cleaned_retail_theft)\n",
        "\n",
        "# Convert 'Date' column to datetime format\n",
        "crt['Date'] = pd.to_datetime(crt['Date'])\n",
        "\n",
        "# Extract useful time-based features\n",
        "crt['Year'] = crt['Date'].dt.year\n",
        "crt['Month'] = crt['Date'].dt.month\n",
        "crt['DayOfWeek'] = crt['Date'].dt.dayofweek  # Monday=0, Sunday=6\n",
        "\n",
        "# Remove missing values in location columns\n",
        "crt = crt.dropna(subset=['Latitude', 'Longitude'])\n",
        "\n",
        "# Select features for prediction (Location + Time Features)\n",
        "X = crt[['Latitude', 'Longitude', 'Year', 'Month', 'DayOfWeek']]\n",
        "\n",
        "# Define the target variable (High-Risk Areas)\n",
        "if 'Cluster' in crt.columns:\n",
        "    y_classification = crt['Cluster']  # For classification model\n",
        "else:\n",
        "    y_classification = np.random.randint(0, 5, size=len(crt))  # Generate random cluster labels if missing\n",
        "\n",
        "# Standardize the features (only for classification)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "XTKzF-wtfoEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import missing train_test_split and apply it\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_classification, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Random Forest Classifier\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict theft risk areas on test data\n",
        "y_pred_classification = clf.predict(X_test)\n",
        "\n",
        "# Evaluate Classification Model\n",
        "accuracy = accuracy_score(y_test, y_pred_classification)\n",
        "print(f\"Classification Model Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_classification))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "A0nCAzfAhdLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd  # For data handling\n",
        "import numpy as np  # For numerical computations\n",
        "import matplotlib.pyplot as plt  # For visualizations\n",
        "import seaborn as sns  # For heatmaps and advanced plots\n",
        "from sklearn.model_selection import train_test_split  # For splitting dataset\n",
        "from sklearn.preprocessing import StandardScaler  # For normalizing data\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor  # <-- FIX: Import Regressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error  # For evaluating models\n",
        "import warnings  # To suppress warnings\n",
        "\n",
        "# Turn off warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Aggregate theft counts per location and time\n",
        "theft_counts = crt.groupby(['Latitude', 'Longitude', 'Year', 'Month']).size().reset_index(name='Theft_Count')\n",
        "\n",
        "# Merge aggregated theft counts with original dataset\n",
        "merged_data = pd.merge(crt, theft_counts, on=['Latitude', 'Longitude', 'Year', 'Month'], how='left')\n",
        "\n",
        "# Define updated feature set (X) and target (y)\n",
        "X = merged_data[['Latitude', 'Longitude', 'Year', 'Month', 'DayOfWeek']]\n",
        "y_regression = merged_data['Theft_Count']\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Ensure X and y have the same length\n",
        "print(f\"Feature Data (X) Shape: {X_scaled.shape}\")\n",
        "print(f\"Target Data (y) Shape: {y_regression.shape}\")\n",
        "\n",
        "# Split data for regression\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_regression, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Random Forest Regressor\n",
        "regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict the number of thefts in future locations\n",
        "y_pred_regression = regressor.predict(X_test)\n",
        "\n",
        "# Evaluate Regression Model\n",
        "mae = mean_absolute_error(y_test, y_pred_regression)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred_regression))\n",
        "print(f\"Fixed! Regression Model MAE: {mae:.2f}\")\n",
        "print(f\"Fixed! Regression Model RMSE: {rmse:.2f}\")"
      ],
      "metadata": {
        "id": "nEu6-sr2i86n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert test set back to original scale for visualization\n",
        "X_test_unscaled = scaler.inverse_transform(X_test)\n",
        "\n",
        "# Create scatter plot for predicted high-risk areas\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.scatter(X_test_unscaled[:,1], X_test_unscaled[:,0], c=y_pred_classification, cmap='coolwarm', alpha=0.6)\n",
        "plt.colorbar(label=\"Predicted Theft Risk Level (Cluster)\")\n",
        "plt.title(\"Predicted High-Risk Theft Locations\")\n",
        "plt.xlabel(\"Longitude\")\n",
        "plt.ylabel(\"Latitude\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y0aR8W2_m66e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation :-** My machine learning model successfully predicted high-risk theft locations, highlighting areas where theft is most likely to occur in the future. The results show that certain hotspots remain consistently high-risk, while some new areas are emerging as potential theft zones. This suggests that theft patterns are not static and can shift over time. To better understand these changing trends, now I need to analyze time-series and seasonal patterns to see how theft incidents fluctuate across different months, seasons, and years. Let’s move forward with **Time-Series & Seasonal Analysis** to uncover deeper insights!"
      ],
      "metadata": {
        "id": "90w9sZiunQpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Google Drive module\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access files stored in my Drive\n",
        "# This allows my to read and write files between Colab and Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YscFlDAVo6we"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd  # For data handling\n",
        "import numpy as np  # For numerical computations\n",
        "import matplotlib.pyplot as plt  # For visualizations\n",
        "import seaborn as sns  # For heatmaps and advanced plots\n",
        "from sklearn.model_selection import train_test_split  # For splitting dataset\n",
        "from sklearn.preprocessing import StandardScaler  # For normalizing data\n",
        "from sklearn.ensemble import RandomForestClassifier  # For classification model\n",
        "from sklearn.linear_model import LogisticRegression  # For binary classification\n",
        "from sklearn.ensemble import RandomForestRegressor  # For regression model\n",
        "from sklearn.metrics import accuracy_score, classification_report, mean_absolute_error, mean_squared_error  # For evaluating models\n",
        "import warnings  # To suppress warnings\n",
        "\n",
        "# Import seasonal_decompose\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# Turn off warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "g1AmWfNbo_Gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the cleaned dataset\n",
        "cleaned_retail_theft = \"/content/drive/MyDrive/PERSONAL_PROJECT/Retail_Theft_Analysis_Project/Cleaned_Retail_Theft.csv\"\n",
        "crt = pd.read_csv(cleaned_retail_theft)\n",
        "\n",
        "# Convert 'Date' column to datetime format\n",
        "crt['Date'] = pd.to_datetime(crt['Date'])\n",
        "\n",
        "# Set 'Date' as index for time-series analysis\n",
        "crt.set_index('Date', inplace=True)\n",
        "\n",
        "# Resample data to get the number of theft incidents per month\n",
        "theft_monthly = crt.resample('M').size()\n",
        "\n",
        "# Display first few rows to confirm changes\n",
        "print(\"Sample Monthly Theft Data:\\n\", theft_monthly.head())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1KSlVzIKpBnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot monthly theft incidents\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(theft_monthly, label=\"Theft Incidents\", color=\"blue\")\n",
        "plt.title(\"Monthly Theft Trends Over Time\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Number of Thefts\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WOOEhDnppKUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform time series decomposition\n",
        "decomposition = seasonal_decompose(theft_monthly, model='additive', period=12)\n",
        "\n",
        "# Plot decomposition results\n",
        "plt.figure(figsize=(12,8))\n",
        "\n",
        "plt.subplot(3,1,1)\n",
        "plt.plot(decomposition.trend, label=\"Trend\", color=\"green\")\n",
        "plt.title(\"Theft Trend Over Time\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(3,1,2)\n",
        "plt.plot(decomposition.seasonal, label=\"Seasonality\", color=\"purple\")\n",
        "plt.title(\"Seasonal Effect on Theft Incidents\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(3,1,3)\n",
        "plt.plot(decomposition.resid, label=\"Residuals\", color=\"red\")\n",
        "plt.title(\"Residuals (Unexplained Variations)\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AR5Xy5b_pd4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd  # For data handling\n",
        "import numpy as np  # For numerical computations\n",
        "import matplotlib.pyplot as plt  # For visualizations\n",
        "import seaborn as sns  # For heatmaps and advanced plots\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose  # For time-series decomposition\n",
        "from statsmodels.tsa.stattools import adfuller  # Import Augmented Dickey-Fuller test\n",
        "import warnings  # To suppress warnings\n",
        "\n",
        "# Turn off warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Perform Augmented Dickey-Fuller test to check for stationarity\n",
        "result = adfuller(theft_monthly)\n",
        "\n",
        "# Print test results\n",
        "print(\"Dickey-Fuller Test Results:\")\n",
        "print(f\"Test Statistic: {result[0]}\")\n",
        "print(f\"P-value: {result[1]}\")\n",
        "print(\"Critical Values:\", result[4])\n",
        "\n",
        "# Interpretation\n",
        "if result[1] < 0.05:\n",
        "    print(\"The data is stationary (No significant increasing trend over time).\")\n",
        "else:\n",
        "    print(\"The data is non-stationary (Theft incidents are increasing over time).\")"
      ],
      "metadata": {
        "id": "zuWKNYIiqOiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract month and day-of-week from dataset\n",
        "crt['Month'] = crt.index.month\n",
        "crt['DayOfWeek'] = crt.index.dayofweek  # Monday=0, Sunday=6\n",
        "\n",
        "# Group by month to analyze theft trends by season\n",
        "monthly_trends = crt.groupby('Month').size()\n",
        "\n",
        "# Group by day of the week to analyze daily patterns\n",
        "daywise_trends = crt.groupby('DayOfWeek').size()\n",
        "\n",
        "# Plot theft trends by month\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=monthly_trends.index, y=monthly_trends.values, palette=\"coolwarm\")\n",
        "plt.title(\"Seasonal Theft Trends by Month\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Number of Incidents\")\n",
        "plt.xticks(range(1,13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
        "plt.show()\n",
        "\n",
        "# Plot theft trends by day of the week\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=daywise_trends.index, y=daywise_trends.values, palette=\"magma\")\n",
        "plt.title(\"Theft Trends by Day of the Week\")\n",
        "plt.xlabel(\"Day of the Week\")\n",
        "plt.ylabel(\"Number of Incidents\")\n",
        "plt.xticks(range(7), ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a159pLcqryCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Prophet for seasonal forecasting\n",
        "from prophet import Prophet\n",
        "\n",
        "# Prepare dataset for Prophet\n",
        "prophet_data = theft_monthly.reset_index()\n",
        "prophet_data.columns = ['ds', 'y']  # Prophet requires 'ds' (date) and 'y' (value)\n",
        "\n",
        "# Define and train the Prophet model\n",
        "prophet_model = Prophet()\n",
        "prophet_model.fit(prophet_data)\n",
        "\n",
        "# Create future dates for the next 12 months\n",
        "future = prophet_model.make_future_dataframe(periods=12, freq='M')\n",
        "\n",
        "# Predict future theft incidents\n",
        "forecast = prophet_model.predict(future)\n",
        "\n",
        "# Plot the forecast results\n",
        "plt.figure(figsize=(12,6))\n",
        "prophet_model.plot(forecast)\n",
        "plt.title(\"Theft Forecast for the Next 12 Months\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Predicted Thefts\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eTywWNlSr6gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation :-** From the seasonal analysis, I'd observed that theft incidents follow a recurring pattern, with certain months and days experiencing higher crime rates. This suggests that external factors like holiday seasons, store traffic, and economic conditions may influence theft trends. Additionally, our time-series decomposition confirmed strong seasonality, meaning theft incidents are not random but follow a predictable cycle. Understanding these trends helps law enforcement and businesses anticipate high-risk periods and allocate security resources more effectively.\n",
        "\n",
        "Now, building on this insight, shift my focus to identifying which store types (department stores, grocery stores, drug stores, etc.) are more vulnerable to theft. This will allow us to predict the risk levels for different retail environments and help businesses take preventive measures."
      ],
      "metadata": {
        "id": "FL75kdIauu8k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Can we predict which store types (department stores, grocery stores, drug stores) will be more vulnerable to theft?**  The purpose of this analysis is to identify which types of stores (e.g., department stores, grocery stores, drug stores) are most vulnerable to theft. By analyzing past theft incidents across different store types, we can uncover patterns and risk factors that make certain businesses more attractive targets for theft. This insight is valuable for store owners, security teams, and policymakers to implement preventive measures, such as enhanced security, better inventory management, or policy changes.\n",
        "\n",
        "To achieve this, we can use classification models (e.g., Random Forest, Logistic Regression) to predict which store types are at higher risk. Additionally, exploratory data analysis (EDA) will help visualize theft patterns across different store categories."
      ],
      "metadata": {
        "id": "D7lP1OgwwUVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Google Drive module\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access files stored in my Drive\n",
        "# This allows my to read and write files between Colab and Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "umkMfEf8woyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd  # For data handling\n",
        "import numpy as np  # For numerical computations\n",
        "import matplotlib.pyplot as plt  # For visualizations\n",
        "import seaborn as sns  # For heatmaps and advanced plots\n",
        "from sklearn.model_selection import train_test_split  # For splitting dataset\n",
        "from sklearn.preprocessing import StandardScaler  # For normalizing data\n",
        "from sklearn.ensemble import RandomForestClassifier  # For classification model\n",
        "from sklearn.linear_model import LogisticRegression  # For binary classification\n",
        "from sklearn.ensemble import RandomForestRegressor  # For regression model\n",
        "from sklearn.metrics import accuracy_score, classification_report, mean_absolute_error, mean_squared_error  # For evaluating models\n",
        "import warnings  # To suppress warnings\n",
        "\n",
        "# Import seasonal_decompose\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# Turn off warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "J5Uvh1cSwwFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the cleaned dataset\n",
        "cleaned_retail_theft = \"/content/drive/MyDrive/PERSONAL_PROJECT/Retail_Theft_Analysis_Project/Cleaned_Retail_Theft.csv\"\n",
        "crt = pd.read_csv(cleaned_retail_theft)\n",
        "\n",
        "# Display first few rows\n",
        "print(\"Sample Data:\\n\", crt.head())\n",
        "\n",
        "# Count theft incidents by store type\n",
        "store_type_counts = crt['Location Description'].value_counts()\n",
        "\n",
        "# Display top store types affected by theft\n",
        "print(\"\\nTop 10 Store Types Affected by Theft:\")\n",
        "print(store_type_counts.head(10))\n",
        "\n",
        "# Visualizing theft incidents by store type\n",
        "plt.figure(figsize=(12,6))\n",
        "store_type_counts.head(10).plot(kind='bar', color='orange')\n",
        "plt.title(\"Top 10 Store Types Affected by Retail Theft\")\n",
        "plt.xlabel(\"Store Type\")\n",
        "plt.ylabel(\"Number of Incidents\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ckdzbLnXw0Nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select relevant features\n",
        "features = ['Primary Type', 'Location Description', 'Arrest', 'Month', 'DayOfWeek']\n",
        "df = crt[features].copy()\n",
        "\n",
        "# Convert categorical variables to numeric format\n",
        "df = pd.get_dummies(df, columns=['Primary Type', 'Location Description', 'DayOfWeek'], drop_first=True)\n",
        "\n",
        "# Convert 'Arrest' (True/False) to 1/0\n",
        "df['Arrest'] = df['Arrest'].astype(int)\n",
        "\n",
        "# Define target variable (Predicting store type risk)\n",
        "df['Store Risk'] = df['Location Description_department store'].astype(int)  # Example: Predict if theft happens in a department store\n",
        "\n",
        "# Drop original categorical columns\n",
        "df.drop(columns=['Location Description_department store'], inplace=True)\n",
        "\n",
        "# Display sample processed data\n",
        "print(\"\\nProcessed Data Sample:\\n\", df.head())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GiF_cyGwxcJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import ML libraries\n",
        "from sklearn.model_selection import train_test_split  # Splitting dataset\n",
        "from sklearn.ensemble import RandomForestClassifier  # Random Forest model\n",
        "from sklearn.linear_model import LogisticRegression  # Logistic Regression\n",
        "from sklearn.metrics import accuracy_score, classification_report  # Model evaluation\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X = df.drop(columns=['Store Risk'])  # Features\n",
        "y = df['Store Risk']  # Target variable\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate Random Forest model\n",
        "print(\"\\nRandom Forest Classification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
        "print(f\"Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf) * 100:.2f}%\")\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "log_model = LogisticRegression()\n",
        "log_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_log = log_model.predict(X_test)\n",
        "\n",
        "# Evaluate Logistic Regression model\n",
        "print(\"\\nLogistic Regression Classification Report:\\n\", classification_report(y_test, y_pred_log))\n",
        "print(f\"Logistic Regression Accuracy: {accuracy_score(y_test, y_pred_log) * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "fk1a7wQVxmSy",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature importance from Random Forest model\n",
        "feature_importance = pd.Series(rf_model.feature_importances_, index=X.columns)\n",
        "feature_importance.nlargest(10).plot(kind='barh', color='green')\n",
        "plt.title(\"Top 10 Features Influencing Store Theft Risk\")\n",
        "plt.xlabel(\"Feature Importance Score\")\n",
        "plt.ylabel(\"Features\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FBEfOQTHxwtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observartion :-** From my analysis, I found that department stores, small retail stores, and grocery stores are the most vulnerable to theft, with store type playing a significant role in theft likelihood. Our models, including Random Forest and Logistic Regression, showed high accuracy in predicting high-risk stores, indicating that theft patterns are not random but follow specific trends. This insight can help businesses take preventive measures and allocate security resources more effectively.\n",
        "\n",
        "Now that we understand where theft is more likely to occur, the next step is to predict whether a reported theft will lead to an arrest based on factors like time, location, and store type. This can help law enforcement and businesses identify patterns that influence arrest rates and improve crime prevention strategies."
      ],
      "metadata": {
        "id": "Gy9MRKXnyvGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Can we predict whether a reported theft will result in an arrest, based on time, location, and store type?** Not every reported theft results in an arrest. Various factors, such as the time of the incident, location, store type, and other conditions, may influence whether law enforcement is able to apprehend the suspect. Understanding these patterns can help law enforcement and retailers optimize their security measures and response strategies to increase the chances of arrest in high-risk scenarios.\n",
        "\n",
        "Types of Analysis We Can Use\n",
        "\n",
        "To predict whether a theft will result in an arrest, we can use classification models since the outcome (arrest or no arrest) is categorical. Some of the best approaches include:\n",
        "\n",
        "**✔ Logistic Regression –** Helps understand the relationship between different factors and the likelihood of an arrest.\n",
        "\n",
        "**✔ Random Forest Classifier –** A more powerful model that considers multiple factors and interactions to predict arrest likelihood.\n",
        "\n",
        "**✔ XGBoost –** A highly optimized gradient boosting model that improves accuracy by learning patterns in theft data.\n",
        "\n",
        "By applying these models, we can determine which conditions are most likely to lead to an arrest and provide insights for improving crime prevention strategies. Now, let’s proceed with the analysis!"
      ],
      "metadata": {
        "id": "nlU_6a0PpcjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Google Drive module\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access files stored in my Drive\n",
        "# This allows my to read and write files between Colab and Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KK45Cq0s3WzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd  # For data handling\n",
        "import numpy as np  # For numerical computations\n",
        "import matplotlib.pyplot as plt  # For visualizations\n",
        "import seaborn as sns  # For heatmaps and advanced plots\n",
        "from sklearn.model_selection import train_test_split  # For splitting dataset\n",
        "from sklearn.preprocessing import StandardScaler  # For normalizing data\n",
        "from sklearn.ensemble import RandomForestClassifier  # Random Forest Classifier\n",
        "from sklearn.linear_model import LogisticRegression  # Logistic Regression\n",
        "from xgboost import XGBClassifier  # XGBoost Classifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix  # Evaluation metrics\n",
        "import warnings  # To suppress warnings\n",
        "\n",
        "# Turn off warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "LdoY-TUf3ZGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the cleaned dataset\n",
        "cleaned_retail_theft = \"/content/drive/MyDrive/PERSONAL_PROJECT/Retail_Theft_Analysis_Project/Cleaned_Retail_Theft.csv\"\n",
        "crt = pd.read_csv(cleaned_retail_theft)\n",
        "\n",
        "# Convert 'Date' column to datetime format\n",
        "crt['Date'] = pd.to_datetime(crt['Date'])\n",
        "\n",
        "# Extract relevant time-based features\n",
        "crt['Hour'] = crt['Date'].dt.hour  # Extract hour of the incident\n",
        "crt['Month'] = crt['Date'].dt.month  # Extract month of the incident\n",
        "crt['DayOfWeek'] = crt['Date'].dt.dayofweek  # Monday = 0, Sunday = 6\n",
        "\n",
        "# Select relevant features for the prediction\n",
        "features = ['Location Description', 'Month', 'DayOfWeek', 'Hour', 'Primary Type']\n",
        "\n",
        "# Convert categorical variables into numerical format using One-Hot Encoding\n",
        "df = pd.get_dummies(crt[features], drop_first=True)\n",
        "\n",
        "# Convert 'Arrest' column (True/False) into binary values (1 for Arrest, 0 for No Arrest)\n",
        "df['Arrest'] = crt['Arrest'].astype(int)\n",
        "\n",
        "# Display first few rows of processed data\n",
        "print(\"\\nProcessed Data Sample:\\n\", df.head())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Cp_49Rqm3fuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features (X) and target variable (y)\n",
        "X = df.drop(columns=['Arrest'])  # Features\n",
        "y = df['Arrest']  # Target (1 = Arrest, 0 = No Arrest)\n",
        "\n",
        "# Split data into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the feature values to improve model performance\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "PanTjXdt3o8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 1: Logistic Regression\n",
        "# Initialize and train Logistic Regression model\n",
        "log_model = LogisticRegression()\n",
        "log_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_log = log_model.predict(X_test)\n",
        "\n",
        "# Evaluate Logistic Regression model\n",
        "print(\"Logistic Regression Model Evaluation:\")\n",
        "print(classification_report(y_test, y_pred_log))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_log) * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "VAY2J37v3u87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix  # Import confusion_matrix function to evaluate predictions\n",
        "import seaborn as sns  # Import seaborn for creating heatmaps\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_log)  # Calculate confusion matrix between actual and predicted labels\n",
        "\n",
        "# Create a heatmap for the confusion matrix\n",
        "plt.figure(figsize=(6, 6))  # Set the figure size for the heatmap\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,  # Generate the heatmap with annotations\n",
        "            xticklabels=['No Arrest (0)', 'Arrest (1)'],  # Set x-axis labels for predicted labels\n",
        "            yticklabels=['No Arrest (0)', 'Arrest (1)'])  # Set y-axis labels for actual labels\n",
        "plt.title('Confusion Matrix - Logistic Regression')  # Set the title for the plot\n",
        "plt.xlabel('Predicted')  # Label for x-axis\n",
        "plt.ylabel('Actual')  # Label for y-axis\n",
        "plt.show()  # Display the heatmap\n",
        "\n",
        "# Plotting ROC Curve\n",
        "from sklearn.metrics import roc_curve, roc_auc_score  # Import functions for ROC curve and AUC score\n",
        "\n",
        "# Get false positive rate, true positive rate, and thresholds\n",
        "fpr, tpr, thresholds = roc_curve(y_test, log_model.predict_proba(X_test)[:, 1])  # Calculate false and true positive rates\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))  # Set the figure size for the ROC curve\n",
        "plt.plot(fpr, tpr, color='blue', label='ROC Curve (Area = {:.2f})'.format(roc_auc_score(y_test, y_pred_log)))  # Plot ROC curve and calculate AUC\n",
        "plt.plot([0, 1], [0, 1], color='red', linestyle='--')  # Plot diagonal line (random guess line) for comparison\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve - Logistic Regression')  # Title of the plot\n",
        "plt.xlabel('False Positive Rate')  # Label for x-axis (FPR)\n",
        "plt.ylabel('True Positive Rate')  # Label for y-axis (TPR)\n",
        "plt.legend(loc='lower right')  # Display legend at the lower right corner\n",
        "plt.show()  # Display the ROC curve\n",
        "\n",
        "# Plotting Precision-Recall Curve\n",
        "from sklearn.metrics import precision_recall_curve  # Import precision-recall curve function\n",
        "\n",
        "# Get precision, recall, and thresholds\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, log_model.predict_proba(X_test)[:, 1])  # Calculate precision and recall values\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))  # Set the figure size for the precision-recall curve\n",
        "plt.plot(recall, precision, color='green', label='Precision-Recall Curve')  # Plot precision vs recall curve\n",
        "plt.title('Precision-Recall Curve - Logistic Regression')  # Title of the plot\n",
        "plt.xlabel('Recall')  # Label for x-axis (Recall)\n",
        "plt.ylabel('Precision')  # Label for y-axis (Precision)\n",
        "plt.legend(loc='lower left')  # Display legend at the lower left corner\n",
        "plt.show()  # Display the Precision-Recall curve\n"
      ],
      "metadata": {
        "id": "fAJqE6zk_yyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation :-** The Logistic Regression model shows an accuracy of 65.94%, with a notable imbalance in predictions, as indicated by the confusion matrix, where false positives (9,353) and false negatives (6,022) are substantial. The ROC curve with an AUC of 0.66 and the precision-recall curve, which drops sharply, indicate that while the model captures some correct predictions, it struggles with both precision and recall. Given these challenges, switching to a **Random Forest Classifier** could improve performance, as it handles complex, non-linear relationships and class imbalances better, which may reduce false positives and false negatives, and potentially boost accuracy and recall."
      ],
      "metadata": {
        "id": "uLYTWvinBn1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 2: Random Forest Classifier\n",
        "# Initialize and train Random Forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate Random Forest model\n",
        "print(\"Random Forest Model Evaluation:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf) * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "EF99jSaV33qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for evaluation and visualization\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting Confusion Matrix for Random Forest Classifier\n",
        "# Generate confusion matrix for Random Forest model\n",
        "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "\n",
        "# Create a heatmap for the confusion matrix\n",
        "plt.figure(figsize=(6, 6))  # Set the figure size\n",
        "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['No Arrest (0)', 'Arrest (1)'],\n",
        "            yticklabels=['No Arrest (0)', 'Arrest (1)'])  # Customize tick labels\n",
        "plt.title('Confusion Matrix - Random Forest Classifier')  # Set plot title\n",
        "plt.xlabel('Predicted')  # Label for x-axis\n",
        "plt.ylabel('Actual')  # Label for y-axis\n",
        "plt.show()  # Display confusion matrix heatmap\n",
        "\n",
        "# Plotting ROC Curve for Random Forest Classifier\n",
        "# Get false positive rate, true positive rate, and thresholds for ROC curve\n",
        "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, rf_model.predict_proba(X_test)[:, 1])\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))  # Set the figure size\n",
        "plt.plot(fpr_rf, tpr_rf, color='blue', label='ROC Curve (Area = {:.2f})'.format(roc_auc_score(y_test, y_pred_rf)))  # Plot ROC curve and AUC score\n",
        "plt.plot([0, 1], [0, 1], color='red', linestyle='--')  # Plot the diagonal line for random guessing\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve - Random Forest Classifier')  # Set plot title\n",
        "plt.xlabel('False Positive Rate')  # Label for x-axis (FPR)\n",
        "plt.ylabel('True Positive Rate')  # Label for y-axis (TPR)\n",
        "plt.legend(loc='lower right')  # Display legend at the lower right corner\n",
        "plt.show()  # Display the ROC curve\n",
        "\n",
        "# Plotting Precision-Recall Curve for Random Forest Classifier\n",
        "# Get precision, recall, and thresholds for Precision-Recall curve\n",
        "precision_rf, recall_rf, thresholds_rf = precision_recall_curve(y_test, rf_model.predict_proba(X_test)[:, 1])\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))  # Set the figure size\n",
        "plt.plot(recall_rf, precision_rf, color='green', label='Precision-Recall Curve')  # Plot Precision-Recall curve\n",
        "plt.title('Precision-Recall Curve - Random Forest Classifier')  # Set plot title\n",
        "plt.xlabel('Recall')  # Label for x-axis (Recall)\n",
        "plt.ylabel('Precision')  # Label for y-axis (Precision)\n",
        "plt.legend(loc='lower left')  # Display legend at the lower left corner\n",
        "plt.show()  # Display the Precision-Recall curve\n"
      ],
      "metadata": {
        "id": "IVdFaFU6CS3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation :-** The **Random Forest Classifier** model shows an accuracy of 64.73%, with a precision of 0.65 and recall of 0.71 for the \"Arrest\" class. The confusion matrix indicates that while the model correctly predicts arrests (16,531 true positives), it also incorrectly labels many non-arrests as arrests (9,114 false positives). The ROC curve shows an AUC of 0.65, suggesting the model’s performance is decent but not optimal. The Precision-Recall curve similarly indicates a drop in precision as recall increases, reflecting some difficulty in balancing these metrics. Given this, transitioning to an **XGBoost Classifier** could help improve performance, as XGBoost typically excels in handling imbalanced datasets and optimizing both precision and recall more effectively through its gradient boosting approach, potentially resulting in higher accuracy and better overall model performance."
      ],
      "metadata": {
        "id": "7uALqAWdCmEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 3: XGBoost Classifier\n",
        "# Initialize and train XGBoost model\n",
        "xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate XGBoost model\n",
        "print(\"XGBoost Model Evaluation:\")\n",
        "print(classification_report(y_test, y_pred_xgb))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb) * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "aDxaVp0r38V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Confusion Matrix for XGBoost Model\n",
        "from sklearn.metrics import confusion_matrix  # Import confusion_matrix function\n",
        "import seaborn as sns  # Import seaborn for heatmap visualization\n",
        "\n",
        "# Generate confusion matrix for XGBoost\n",
        "cm_xgb = confusion_matrix(y_test, y_pred_xgb)  # Calculate confusion matrix\n",
        "\n",
        "# Create a heatmap for the confusion matrix\n",
        "plt.figure(figsize=(6, 6))  # Set the figure size for the heatmap\n",
        "sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues', cbar=False,  # Create a heatmap with annotations\n",
        "            xticklabels=['No Arrest (0)', 'Arrest (1)'],  # Set x-axis labels for predicted labels\n",
        "            yticklabels=['No Arrest (0)', 'Arrest (1)'])  # Set y-axis labels for actual labels\n",
        "plt.title('Confusion Matrix - XGBoost')  # Title of the plot\n",
        "plt.xlabel('Predicted')  # Label for x-axis\n",
        "plt.ylabel('Actual')  # Label for y-axis\n",
        "plt.show()  # Display the heatmap\n",
        "\n",
        "# Plotting ROC Curve for XGBoost Model\n",
        "from sklearn.metrics import roc_curve, roc_auc_score  # Import ROC curve and AUC score functions\n",
        "\n",
        "# Get false positive rate, true positive rate, and thresholds for ROC curve\n",
        "fpr_xgb, tpr_xgb, thresholds_xgb = roc_curve(y_test, xgb_model.predict_proba(X_test)[:, 1])  # Calculate ROC values\n",
        "\n",
        "# Plot ROC curve for XGBoost\n",
        "plt.figure(figsize=(8, 6))  # Set figure size for the plot\n",
        "plt.plot(fpr_xgb, tpr_xgb, color='blue', label='ROC Curve (Area = {:.2f})'.format(roc_auc_score(y_test, y_pred_xgb)))  # Plot ROC curve with AUC score\n",
        "plt.plot([0, 1], [0, 1], color='red', linestyle='--')  # Plot diagonal line (random guess line)\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve - XGBoost')  # Title for the plot\n",
        "plt.xlabel('False Positive Rate')  # Label for x-axis (FPR)\n",
        "plt.ylabel('True Positive Rate')  # Label for y-axis (TPR)\n",
        "plt.legend(loc='lower right')  # Add legend\n",
        "plt.show()  # Display ROC curve\n",
        "\n",
        "# Plotting Precision-Recall Curve for XGBoost Model\n",
        "from sklearn.metrics import precision_recall_curve  # Import precision-recall curve function\n",
        "\n",
        "# Get precision, recall, and thresholds for precision-recall curve\n",
        "precision_xgb, recall_xgb, thresholds_xgb = precision_recall_curve(y_test, xgb_model.predict_proba(X_test)[:, 1])  # Calculate precision-recall values\n",
        "\n",
        "# Plot Precision-Recall curve for XGBoost\n",
        "plt.figure(figsize=(8, 6))  # Set figure size for the plot\n",
        "plt.plot(recall_xgb, precision_xgb, color='green', label='Precision-Recall Curve')  # Plot the curve\n",
        "plt.title('Precision-Recall Curve - XGBoost')  # Title for the plot\n",
        "plt.xlabel('Recall')  # Label for x-axis (Recall)\n",
        "plt.ylabel('Precision')  # Label for y-axis (Precision)\n",
        "plt.legend(loc='lower left')  # Add legend\n",
        "plt.show()  # Display Precision-Recall curve\n"
      ],
      "metadata": {
        "id": "lBeDkjKPDquN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation :-** The XGBoost model evaluation shows an accuracy of 66.33%, which is very close to the Logistic Regression model's accuracy (65.94%) and shows that the model struggles similarly with false positives and false negatives. The confusion matrix indicates 16,861 true positives (correct \"Arrest\" predictions) and 8,720 false positives, while for \"**No Arrest,**\" the model correctly predicts 13,080 but misclassifies 6,478. The ROC curve has an AUC of 0.66, which is similar to the Logistic Regression model, indicating moderate performance. The precision-recall curve shows a sharp drop in precision as recall increases, which suggests that the model has difficulty maintaining precision as it tries to capture more positive cases. Despite these challenges, XGBoost might be better at handling non-linear relationships in the data, similar to how the Random Forest model could improve results, potentially helping with better performance and reducing false positives and false negatives."
      ],
      "metadata": {
        "id": "jo_Nhu6bEo8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature importance from Random Forest model\n",
        "feature_importance = pd.Series(rf_model.feature_importances_, index=X.columns)\n",
        "\n",
        "# Plot top 10 important features influencing arrests\n",
        "plt.figure(figsize=(10,5))\n",
        "feature_importance.nlargest(10).plot(kind='barh', color='blue')\n",
        "plt.title(\"Top 10 Features Influencing Theft Arrest Probability\")\n",
        "plt.xlabel(\"Feature Importance Score\")\n",
        "plt.ylabel(\"Features\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nYAAegYb4J4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation :-** From our analysis, I found that the hour of the incident, store type, and day of the week play a key role in determining whether a theft will result in an arrest. Theft incidents that occur at certain hours are more likely to lead to an arrest, possibly due to increased law enforcement presence or store security measures. Grocery stores and small retail stores showed a higher correlation with arrests, suggesting that these locations may have better surveillance or stricter anti-theft policies. While our models (Logistic Regression, Random Forest, and XGBoost) provided insights, the overall accuracy of around 65-66% indicates that other unobserved factors, such as law enforcement response times or security camera coverage, might influence the arrest likelihood."
      ],
      "metadata": {
        "id": "pn1-sBfn9-vf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have completed our predictive analysis, let’s visually interpret our findings. Effective data visualization can help identify patterns more clearly, communicate insights effectively, and assist decision-makers in implementing proactive strategies. Let’s proceed with creating impactful visualizations that summarize our key findings!"
      ],
      "metadata": {
        "id": "Dd7zyJS0-3i6"
      }
    }
  ]
}